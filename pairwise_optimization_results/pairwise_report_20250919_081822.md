# Pairwise Cross-Validation Report

Generated: 2025-09-19 08:18:22

## Executive Summary

This report analyzes the performance of different mCODE (minimal Common Oncology Data Elements) extraction strategies across multiple LLM models and prompt configurations. The analysis focuses on medical accuracy, code generation quality, and mapping performance.

## Summary

- **Total Comparisons**: 10
- **Successful Comparisons**: 10
- **Success Rate**: 100.0%
- **Unique Configuration Pairs**: 10

## Prompt Strategy Analysis

### SNOMED-Enabled Prompts ‚≠ê
**Strategy**: Integrated SNOMED CT code reference table with evidence-based extraction
**Key Features**:
- Pre-defined SNOMED CT codes for common cancer conditions, treatments, and demographics
- Generates actual medical codes instead of null/placeholder values
- Higher code coverage and medical accuracy
- Maintains strict evidence-based approach

### Evidence-Based Prompts üîç
**Strategy**: Strict fidelity to source text with conservative mapping
**Key Features**:
- Only extracts information explicitly stated in source
- No inference or extrapolation
- Prioritizes accuracy over completeness

### Other Prompts üìù
**Strategy**: Various approaches including structured, comprehensive, and minimal extraction
**Key Features**:
- Different levels of detail and structure
- Varying approaches to medical terminology handling

## Overall Metrics

| Metric | Mean | Median | Std Dev | Min | Max |
|--------|------|--------|---------|-----|-----|
| mapping_jaccard_similarity | 0.095 | 0.051 | 0.135 | 0.000 | 0.464 |
| mapping_precision | 0.147 | 0.089 | 0.164 | 0.000 | 0.565 |
| mapping_recall | 0.172 | 0.124 | 0.205 | 0.000 | 0.722 |
| mapping_f1_score | 0.154 | 0.097 | 0.181 | 0.000 | 0.634 |
| mapping_true_positives | 2.800 | 1.500 | 3.736 | 0.000 | 13.000 |
| mapping_false_positives | 15.500 | 13.500 | 5.191 | 10.000 | 23.000 |
| mapping_false_negatives | 13.200 | 13.500 | 6.161 | 5.000 | 22.000 |
| gold_mappings_count | 18.800 | 22.000 | 7.131 | 9.000 | 29.000 |
| comp_mappings_count | 21.200 | 23.000 | 6.941 | 11.000 | 29.000 |
| gold_avg_confidence | 0.930 | 0.937 | 0.052 | 0.850 | 1.000 |
| comp_avg_confidence | 0.918 | 0.938 | 0.051 | 0.841 | 0.986 |
| gold_compliance_score | 0.400 | 0.333 | 0.141 | 0.333 | 0.667 |
| comp_compliance_score | 0.333 | 0.333 | 0.000 | 0.333 | 0.333 |

## Key Findings

### Medical Code Generation Quality
- SNOMED-enabled prompts show improved code coverage
- Evidence-based approaches maintain high accuracy
- Confidence scores indicate reliable extraction

### Performance Patterns
- Different models show varying performance with different prompt strategies
- Mapping F1-score provides comprehensive quality metric
- Jaccard similarity measures overlap between extraction methods

