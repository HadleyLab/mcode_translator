{
  "summary": {
    "total_comparisons": 50,
    "successful_comparisons": 50,
    "success_rate": 1.0,
    "unique_config_pairs": 50
  },
  "configuration_analysis": {
    "direct_mcode_simple_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.15789473684210525,
        "median": 0.15789473684210525,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.2553191489361702,
        "median": 0.2553191489361702,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.2926829268292683,
        "median": 0.2926829268292683,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.2727272727272727,
        "median": 0.2727272727272727,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 12,
        "median": 12,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 35,
        "median": 35,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 29,
        "median": 29,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 53,
        "median": 53,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8650943396226415,
        "median": 0.8650943396226415,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.046511627906976744,
        "median": 0.046511627906976744,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.07692307692307693,
        "median": 0.07692307692307693,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.08888888888888889,
        "median": 0.08888888888888889,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9021739130434783,
        "median": 0.9021739130434783,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_simple_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.10256410256410256,
        "median": 0.10256410256410256,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.26666666666666666,
        "median": 0.26666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.18604651162790697,
        "median": 0.18604651162790697,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 11,
        "median": 11,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.28125,
        "median": 0.28125,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.47368421052631576,
        "median": 0.47368421052631576,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.4090909090909091,
        "median": 0.4090909090909091,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.43902439024390244,
        "median": 0.43902439024390244,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 28,
        "median": 28,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9107142857142857,
        "median": 0.9107142857142857,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.2857142857142857,
        "median": 0.2857142857142857,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.19047619047619047,
        "median": 0.19047619047619047,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9764705882352942,
        "median": 0.9764705882352942,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_simple_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.046511627906976744,
        "median": 0.046511627906976744,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.07692307692307693,
        "median": 0.07692307692307693,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.08888888888888889,
        "median": 0.08888888888888889,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8913043478260869,
        "median": 0.8913043478260869,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_simple_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.11904761904761904,
        "median": 0.11904761904761904,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.2631578947368421,
        "median": 0.2631578947368421,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.17857142857142858,
        "median": 0.17857142857142858,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.2127659574468085,
        "median": 0.2127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 5,
        "median": 5,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9352941176470588,
        "median": 0.9352941176470588,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.030303030303030304,
        "median": 0.030303030303030304,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0425531914893617,
        "median": 0.0425531914893617,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.09523809523809523,
        "median": 0.09523809523809523,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.0588235294117647,
        "median": 0.0588235294117647,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 45,
        "median": 45,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 19,
        "median": 19,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 55,
        "median": 55,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.7354545454545455,
        "median": 0.7354545454545455,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_deepseek-coder_vs_direct_mcode_deepseek-chat": {
      "mapping_jaccard_similarity": {
        "mean": 0.352112676056338,
        "median": 0.352112676056338,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.5102040816326531,
        "median": 0.5102040816326531,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.5319148936170213,
        "median": 0.5319148936170213,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.5208333333333334,
        "median": 0.5208333333333334,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 22,
        "median": 22,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 54,
        "median": 54,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8351851851851853,
        "median": 0.8351851851851853,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_simple_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.0136986301369863,
        "median": 0.0136986301369863,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.037037037037037035,
        "median": 0.037037037037037035,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.027027027027027025,
        "median": 0.027027027027027025,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 26,
        "median": 26,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 50,
        "median": 50,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.358,
        "median": 0.358,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_comprehensive_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.06896551724137931,
        "median": 0.06896551724137931,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.12903225806451615,
        "median": 0.12903225806451615,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9029411764705882,
        "median": 0.9029411764705882,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_comprehensive_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02702702702702703,
        "median": 0.02702702702702703,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05263157894736841,
        "median": 0.05263157894736841,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 27,
        "median": 27,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9423076923076923,
        "median": 0.9423076923076923,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_comprehensive_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0425531914893617,
        "median": 0.0425531914893617,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.18181818181818182,
        "median": 0.18181818181818182,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.06896551724137931,
        "median": 0.06896551724137931,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 45,
        "median": 45,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9142857142857144,
        "median": 0.9142857142857144,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_comprehensive_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.03125,
        "median": 0.03125,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.060606060606060615,
        "median": 0.060606060606060615,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 6,
        "median": 6,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 12,
        "median": 12,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9041666666666667,
        "median": 0.9041666666666667,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_comprehensive_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.016666666666666666,
        "median": 0.016666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.07142857142857142,
        "median": 0.07142857142857142,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.032786885245901634,
        "median": 0.032786885245901634,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 59,
        "median": 59,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.6728813559322034,
        "median": 0.6728813559322034,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_minimal_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.06060606060606061,
        "median": 0.06060606060606061,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.125,
        "median": 0.125,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.11428571428571428,
        "median": 0.11428571428571428,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 21,
        "median": 21,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8500000000000001,
        "median": 0.8500000000000001,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_minimal_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.047619047619047616,
        "median": 0.047619047619047616,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0851063829787234,
        "median": 0.0851063829787234,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.0975609756097561,
        "median": 0.0975609756097561,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.09090909090909091,
        "median": 0.09090909090909091,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 43,
        "median": 43,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 37,
        "median": 37,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 43,
        "median": 43,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.7895348837209303,
        "median": 0.7895348837209303,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_minimal_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.021739130434782608,
        "median": 0.021739130434782608,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.047619047619047616,
        "median": 0.047619047619047616,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.0425531914893617,
        "median": 0.0425531914893617,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 20,
        "median": 20,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.89,
        "median": 0.89,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_minimal_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.06521739130434782,
        "median": 0.06521739130434782,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.10714285714285714,
        "median": 0.10714285714285714,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.12244897959183672,
        "median": 0.12244897959183672,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 3,
        "median": 3,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 18,
        "median": 18,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9456521739130435,
        "median": 0.9456521739130435,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_structured_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.17857142857142858,
        "median": 0.17857142857142858,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.2631578947368421,
        "median": 0.2631578947368421,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.35714285714285715,
        "median": 0.35714285714285715,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.30303030303030304,
        "median": 0.30303030303030304,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 5,
        "median": 5,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 19,
        "median": 19,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9657894736842106,
        "median": 0.9657894736842106,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_minimal_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.010638297872340425,
        "median": 0.010638297872340425,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.020833333333333332,
        "median": 0.020833333333333332,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.021052631578947368,
        "median": 0.021052631578947368,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 47,
        "median": 47,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 80,
        "median": 80,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.0,
        "median": 0.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.0,
        "median": 0.0,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_structured_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.07894736842105263,
        "median": 0.07894736842105263,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.10714285714285714,
        "median": 0.10714285714285714,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.23076923076923078,
        "median": 0.23076923076923078,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.14634146341463414,
        "median": 0.14634146341463414,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 3,
        "median": 3,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 15,
        "median": 15,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_structured_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.06153846153846154,
        "median": 0.06153846153846154,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0851063829787234,
        "median": 0.0851063829787234,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.18181818181818182,
        "median": 0.18181818181818182,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.11594202898550723,
        "median": 0.11594202898550723,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 43,
        "median": 43,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 18,
        "median": 18,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 43,
        "median": 43,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8976744186046512,
        "median": 0.8976744186046512,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_structured_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.030303030303030304,
        "median": 0.030303030303030304,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.125,
        "median": 0.125,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.058823529411764705,
        "median": 0.058823529411764705,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 7,
        "median": 7,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9115384615384615,
        "median": 0.9115384615384615,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_structured_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.0196078431372549,
        "median": 0.0196078431372549,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.2,
        "median": 0.2,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.03846153846153846,
        "median": 0.03846153846153846,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 20,
        "median": 20,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.0,
        "median": 0.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_optimization_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.10344827586206896,
        "median": 0.10344827586206896,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.15789473684210525,
        "median": 0.15789473684210525,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.23076923076923078,
        "median": 0.23076923076923078,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.18749999999999997,
        "median": 0.18749999999999997,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 3,
        "median": 3,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 16,
        "median": 16,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 21,
        "median": 21,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8547619047619047,
        "median": 0.8547619047619047,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_optimization_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.043478260869565216,
        "median": 0.043478260869565216,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.07142857142857142,
        "median": 0.07142857142857142,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.08333333333333333,
        "median": 0.08333333333333333,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 26,
        "median": 26,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 18,
        "median": 18,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 22,
        "median": 22,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.865909090909091,
        "median": 0.865909090909091,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_optimization_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02857142857142857,
        "median": 0.02857142857142857,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05555555555555555,
        "median": 0.05555555555555555,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8999999999999999,
        "median": 0.8999999999999999,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_optimization_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.08695652173913043,
        "median": 0.08695652173913043,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.1276595744680851,
        "median": 0.1276595744680851,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.21428571428571427,
        "median": 0.21428571428571427,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.16,
        "median": 0.16,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 6,
        "median": 6,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 41,
        "median": 41,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 22,
        "median": 22,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 42,
        "median": 42,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.7845238095238096,
        "median": 0.7845238095238096,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_improved_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.06896551724137931,
        "median": 0.06896551724137931,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.10526315789473684,
        "median": 0.10526315789473684,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.12903225806451615,
        "median": 0.12903225806451615,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 17,
        "median": 17,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9147058823529413,
        "median": 0.9147058823529413,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_optimization_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.017857142857142856,
        "median": 0.017857142857142856,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.03508771929824561,
        "median": 0.03508771929824561,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.0,
        "median": 0.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_improved_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.03508771929824561,
        "median": 0.03508771929824561,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0425531914893617,
        "median": 0.0425531914893617,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.06779661016949153,
        "median": 0.06779661016949153,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 45,
        "median": 45,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9357142857142857,
        "median": 0.9357142857142857,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_improved_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02564102564102564,
        "median": 0.02564102564102564,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.08333333333333333,
        "median": 0.08333333333333333,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05,
        "median": 0.05,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 27,
        "median": 27,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 11,
        "median": 11,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9249999999999999,
        "median": 0.9249999999999999,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_improved_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02857142857142857,
        "median": 0.02857142857142857,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05555555555555555,
        "median": 0.05555555555555555,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8807692307692307,
        "median": 0.8807692307692307,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_improved_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.01818181818181818,
        "median": 0.01818181818181818,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.1111111111111111,
        "median": 0.1111111111111111,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 8,
        "median": 8,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 40,
        "median": 40,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.8150000000000001,
        "median": 0.8150000000000001,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.12121212121212122,
        "median": 0.12121212121212122,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.21052631578947367,
        "median": 0.21052631578947367,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.2222222222222222,
        "median": 0.2222222222222222,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.21621621621621623,
        "median": 0.21621621621621623,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 15,
        "median": 15,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 19,
        "median": 19,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.05970149253731343,
        "median": 0.05970149253731343,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0851063829787234,
        "median": 0.0851063829787234,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.11267605633802817,
        "median": 0.11267605633802817,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 4,
        "median": 4,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 43,
        "median": 43,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 20,
        "median": 20,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 44,
        "median": 44,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9954545454545454,
        "median": 0.9954545454545454,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.1,
        "median": 0.1,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.17857142857142858,
        "median": 0.17857142857142858,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.18518518518518517,
        "median": 0.18518518518518517,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.18181818181818182,
        "median": 0.18181818181818182,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 5,
        "median": 5,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 22,
        "median": 22,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 32,
        "median": 32,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.08571428571428572,
        "median": 0.08571428571428572,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.11538461538461539,
        "median": 0.11538461538461539,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.25,
        "median": 0.25,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.15789473684210525,
        "median": 0.15789473684210525,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 3,
        "median": 3,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 23,
        "median": 23,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 9,
        "median": 9,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 19,
        "median": 19,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9894736842105264,
        "median": 0.9894736842105264,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_concise_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.03773584905660377,
        "median": 0.03773584905660377,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.0425531914893617,
        "median": 0.0425531914893617,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.25,
        "median": 0.25,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.07272727272727272,
        "median": 0.07272727272727272,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 2,
        "median": 2,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 45,
        "median": 45,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 6,
        "median": 6,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 16,
        "median": 16,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_concise_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.041666666666666664,
        "median": 0.041666666666666664,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.05263157894736842,
        "median": 0.05263157894736842,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.08,
        "median": 0.08,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 18,
        "median": 18,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 5,
        "median": 5,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 8,
        "median": 8,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.011363636363636364,
        "median": 0.011363636363636364,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.023809523809523808,
        "median": 0.023809523809523808,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.022471910112359546,
        "median": 0.022471910112359546,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 41,
        "median": 41,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 102,
        "median": 102,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9705882352941176,
        "median": 0.9705882352941176,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_concise_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02564102564102564,
        "median": 0.02564102564102564,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.08333333333333333,
        "median": 0.08333333333333333,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05,
        "median": 0.05,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 27,
        "median": 27,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 11,
        "median": 11,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 16,
        "median": 16,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9875,
        "median": 0.9875,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_concise_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.02631578947368421,
        "median": 0.02631578947368421,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.07692307692307693,
        "median": 0.07692307692307693,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.05128205128205129,
        "median": 0.05128205128205129,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 12,
        "median": 12,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 16,
        "median": 16,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.99375,
        "median": 0.99375,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_concise_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.017543859649122806,
        "median": 0.017543859649122806,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.09090909090909091,
        "median": 0.09090909090909091,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.034482758620689655,
        "median": 0.034482758620689655,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 10,
        "median": 10,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 41,
        "median": 41,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_with_codes_deepseek-chat_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.04,
        "median": 0.04,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.05263157894736842,
        "median": 0.05263157894736842,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.07692307692307693,
        "median": 0.07692307692307693,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 18,
        "median": 18,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 6,
        "median": 6,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 11,
        "median": 11,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 24,
        "median": 24,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.990909090909091,
        "median": 0.990909090909091,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8770833333333333,
        "median": 0.8770833333333333,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_with_codes_gpt-4-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.024390243902439025,
        "median": 0.024390243902439025,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.03571428571428571,
        "median": 0.03571428571428571,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.07142857142857142,
        "median": 0.07142857142857142,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.047619047619047616,
        "median": 0.047619047619047616,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 27,
        "median": 27,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 13,
        "median": 13,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 21,
        "median": 21,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 34,
        "median": 34,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 1.0,
        "median": 1.0,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.9617647058823531,
        "median": 0.9617647058823531,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_with_codes_deepseek-coder_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.019230769230769232,
        "median": 0.019230769230769232,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.16666666666666666,
        "median": 0.16666666666666666,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.03773584905660377,
        "median": 0.03773584905660377,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 5,
        "median": 5,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 20,
        "median": 20,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 51,
        "median": 51,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.985,
        "median": 0.985,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.85,
        "median": 0.85,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_with_codes_gpt-4o_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.03125,
        "median": 0.03125,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.038461538461538464,
        "median": 0.038461538461538464,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.14285714285714285,
        "median": 0.14285714285714285,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.060606060606060615,
        "median": 0.060606060606060615,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 25,
        "median": 25,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 6,
        "median": 6,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 14,
        "median": 14,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 31,
        "median": 31,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9928571428571429,
        "median": 0.9928571428571429,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8951612903225806,
        "median": 0.8951612903225806,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    },
    "direct_mcode_evidence_based_with_codes_gpt-3.5-turbo_vs_direct_mcode_deepseek-coder": {
      "mapping_jaccard_similarity": {
        "mean": 0.017241379310344827,
        "median": 0.017241379310344827,
        "stdev": 0,
        "count": 1
      },
      "mapping_precision": {
        "mean": 0.02127659574468085,
        "median": 0.02127659574468085,
        "stdev": 0,
        "count": 1
      },
      "mapping_recall": {
        "mean": 0.08333333333333333,
        "median": 0.08333333333333333,
        "stdev": 0,
        "count": 1
      },
      "mapping_f1_score": {
        "mean": 0.03389830508474576,
        "median": 0.03389830508474576,
        "stdev": 0,
        "count": 1
      },
      "mapping_true_positives": {
        "mean": 1,
        "median": 1,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_positives": {
        "mean": 46,
        "median": 46,
        "stdev": 0,
        "count": 1
      },
      "mapping_false_negatives": {
        "mean": 11,
        "median": 11,
        "stdev": 0,
        "count": 1
      },
      "gold_mappings_count": {
        "mean": 35,
        "median": 35,
        "stdev": 0,
        "count": 1
      },
      "comp_mappings_count": {
        "mean": 66,
        "median": 66,
        "stdev": 0,
        "count": 1
      },
      "gold_avg_confidence": {
        "mean": 0.9799999999999999,
        "median": 0.9799999999999999,
        "stdev": 0,
        "count": 1
      },
      "comp_avg_confidence": {
        "mean": 0.8681818181818182,
        "median": 0.8681818181818182,
        "stdev": 0,
        "count": 1
      },
      "gold_compliance_score": {
        "mean": 0.6666666666666666,
        "median": 0.6666666666666666,
        "stdev": 0,
        "count": 1
      },
      "comp_compliance_score": {
        "mean": 0.3333333333333333,
        "median": 0.3333333333333333,
        "stdev": 0,
        "count": 1
      }
    }
  },
  "overall_metrics": {
    "mapping_jaccard_similarity": {
      "mean": 0.06250670045414568,
      "median": 0.038867924528301886,
      "stdev": 0.06536269426094517,
      "min": 0.010638297872340425,
      "max": 0.352112676056338
    },
    "mapping_precision": {
      "mean": 0.09504546085813083,
      "median": 0.04759238521836506,
      "stdev": 0.10457833818867586,
      "min": 0.02127659574468085,
      "max": 0.5102040816326531
    },
    "mapping_recall": {
      "mean": 0.15989010472331286,
      "median": 0.14285714285714285,
      "stdev": 0.09789779897407458,
      "min": 0.020833333333333332,
      "max": 0.5319148936170213
    },
    "mapping_f1_score": {
      "mean": 0.11158599204546663,
      "median": 0.07482517482517483,
      "stdev": 0.10117690467064569,
      "min": 0.021052631578947368,
      "max": 0.5208333333333334
    },
    "mapping_true_positives": {
      "mean": 2.92,
      "median": 2.0,
      "stdev": 3.8589413796310215,
      "min": 1,
      "max": 25
    },
    "mapping_false_positives": {
      "mean": 30.52,
      "median": 25.0,
      "stdev": 11.765116199327291,
      "min": 10,
      "max": 46
    },
    "mapping_false_negatives": {
      "mean": 14.5,
      "median": 11.0,
      "stdev": 9.11659173693542,
      "min": 4,
      "max": 47
    },
    "gold_mappings_count": {
      "mean": 27.84,
      "median": 20.5,
      "stdev": 18.828289614192744,
      "min": 8,
      "max": 102
    },
    "comp_mappings_count": {
      "mean": 41.26,
      "median": 34.0,
      "stdev": 15.449469973076086,
      "min": 24,
      "max": 66
    },
    "gold_avg_confidence": {
      "mean": 0.8546991952661318,
      "median": 0.9144957983193278,
      "stdev": 0.24341621937669283,
      "min": 0.0,
      "max": 1.0
    },
    "comp_avg_confidence": {
      "mean": 0.8901419332477207,
      "median": 0.8770833333333333,
      "stdev": 0.03927935255214733,
      "min": 0.8351851851851853,
      "max": 0.9617647058823531
    },
    "gold_compliance_score": {
      "mean": 0.41333333333333333,
      "median": 0.3333333333333333,
      "stdev": 0.1724835264407127,
      "min": 0.0,
      "max": 1.0
    },
    "comp_compliance_score": {
      "mean": 0.39999999999999997,
      "median": 0.3333333333333333,
      "stdev": 0.13468700594029476,
      "min": 0.3333333333333333,
      "max": 0.6666666666666666
    }
  }
}