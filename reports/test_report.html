<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">test_report.html</title>
      <style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
    
  </head>
  <body>
    <h1 id="title">test_report.html</h1>
    <p>Report generated on 17-Sep-2025 at 09:39:17 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">101 tests took 00:00:13.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" />
            <span class="failed">54 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" />
            <span class="passed">47 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" />
            <span class="skipped">10 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" disabled/>
            <span class="xfailed">0 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" />
            <span class="error">2 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled/>
            <span class="rerun">0 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.10.18&#34;, &#34;Platform&#34;: &#34;macOS-10.16-x86_64-i386-64bit&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;8.4.1&#34;, &#34;pluggy&#34;: &#34;1.6.0&#34;}, &#34;Plugins&#34;: {&#34;html&#34;: &#34;4.1.1&#34;, &#34;asyncio&#34;: &#34;1.1.0&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;, &#34;anyio&#34;: &#34;4.10.0&#34;, &#34;benchmark&#34;: &#34;5.1.0&#34;, &#34;cov&#34;: &#34;7.0.0&#34;}}, &#34;tests&#34;: {&#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_mcode_pipeline_with_real_trial_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_mcode_pipeline_with_real_trial_data::setup&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_mcode_pipeline_with_real_trial_data::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 39, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_trial_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_trial_data::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_trial_data::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 59, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_patient_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_patient_data::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_summarizer_with_real_patient_data::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 70, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pipeline_with_memory_storage&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pipeline_with_memory_storage::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pipeline_with_memory_storage::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 79, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_data_flow_coordinator_integration&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_data_flow_coordinator_integration::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_data_flow_coordinator_integration::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 104, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_config_loading_integration&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_config_loading_integration::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_config_loading_integration::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 129, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_api_manager_with_cache&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_api_manager_with_cache::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_api_manager_with_cache::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 146, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_patient_generator_with_real_archive&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_patient_generator_with_real_archive::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_patient_generator_with_real_archive::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 169, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pattern_manager_loading&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pattern_manager_loading::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_pattern_manager_loading::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 184, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_token_tracker_operations&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_token_tracker_operations::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/integration/test_pipeline_integration.py::TestPipelineIntegration::test_token_tracker_operations::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;/Users/idrdex/Documents/mcode_translator/tests/integration/test_pipeline_integration.py&amp;#x27;, 202, &amp;#x27;Skipped: Live tests disabled. Set ENABLE_LIVE_TESTS=true to run.&amp;#x27;)\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_trial_summary_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_trial_summary_performance&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_trial_summary_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_performance.TestPerformanceBenchmarks object at 0x10ad61090&amp;gt;\nbenchmark = &amp;lt;pytest_benchmark.fixture.BenchmarkFixture object at 0x10ae49db0&amp;gt;\nlarge_trial_data = {&amp;#x27;conditions&amp;#x27;: [&amp;#x27;Cancer&amp;#x27;, &amp;#x27;Breast Cancer&amp;#x27;, &amp;#x27;Lung Cancer&amp;#x27;, &amp;#x27;Prostate Cancer&amp;#x27;, &amp;#x27;Cancer&amp;#x27;, &amp;#x27;Breast Cancer&amp;#x27;, ...], &amp;#x27;eligibi...&amp;#x27;: &amp;#x27;State 4&amp;#x27;}, {&amp;#x27;city&amp;#x27;: &amp;#x27;City 5&amp;#x27;, &amp;#x27;country&amp;#x27;: &amp;#x27;United States&amp;#x27;, &amp;#x27;facility&amp;#x27;: &amp;#x27;Hospital 5&amp;#x27;, &amp;#x27;state&amp;#x27;: &amp;#x27;State 0&amp;#x27;}, ...], ...}\n\n    def test_summarizer_trial_summary_performance(self, benchmark, large_trial_data):\n        &amp;quot;&amp;quot;&amp;quot;Benchmark trial summary generation performance.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        def run_summary():\n            return summarizer.create_trial_summary(large_trial_data)\n    \n&amp;gt;       result = benchmark(run_summary)\n\ntests/performance/test_performance.py:90: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:156: in __call__\n    return self._raw(function_to_benchmark, *args, **kwargs)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:180: in _raw\n    duration, iterations, loops_range = self._calibrate_timer(runner)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:318: in _calibrate_timer\n    duration = runner(loops_range)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:109: in runner\n    function_to_benchmark(*args, **kwargs)\ntests/performance/test_performance.py:88: in run_summary\n    return summarizer.create_trial_summary(large_trial_data)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10ae4a1a0&amp;gt;\ntrial_data = {&amp;#x27;conditions&amp;#x27;: [&amp;#x27;Cancer&amp;#x27;, &amp;#x27;Breast Cancer&amp;#x27;, &amp;#x27;Lung Cancer&amp;#x27;, &amp;#x27;Prostate Cancer&amp;#x27;, &amp;#x27;Cancer&amp;#x27;, &amp;#x27;Breast Cancer&amp;#x27;, ...], &amp;#x27;eligibi...&amp;#x27;: &amp;#x27;State 4&amp;#x27;}, {&amp;#x27;city&amp;#x27;: &amp;#x27;City 5&amp;#x27;, &amp;#x27;country&amp;#x27;: &amp;#x27;United States&amp;#x27;, &amp;#x27;facility&amp;#x27;: &amp;#x27;Hospital 5&amp;#x27;, &amp;#x27;state&amp;#x27;: &amp;#x27;State 0&amp;#x27;}, ...], ...}\n\n    def create_trial_summary(self, trial_data: Dict[str, Any]) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive clinical trial summary in natural language format.\n    \n        Args:\n            trial_data: Clinical trial data from ClinicalTrials.gov API\n    \n        Returns:\n            str: Natural language summary of the clinical trial\n    \n        Raises:\n            ValueError: If trial data is missing required fields\n        &amp;quot;&amp;quot;&amp;quot;\n        if not trial_data or not trial_data.get(&amp;quot;protocolSection&amp;quot;):\n&amp;gt;           raise ValueError(&amp;quot;Trial data is missing or not in the expected format.&amp;quot;)\nE           ValueError: Trial data is missing or not in the expected format.\n\nsrc/services/summarizer.py:326: ValueError\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_patient_summary_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_patient_summary_performance&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_summarizer_patient_summary_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_performance.TestPerformanceBenchmarks object at 0x10ad61240&amp;gt;\nbenchmark = &amp;lt;pytest_benchmark.fixture.BenchmarkFixture object at 0x10aed7be0&amp;gt;\nlarge_patient_bundle = {&amp;#x27;entry&amp;#x27;: [{&amp;#x27;resource&amp;#x27;: {&amp;#x27;birthDate&amp;#x27;: &amp;#x27;1900-01-01&amp;#x27;, &amp;#x27;gender&amp;#x27;: &amp;#x27;female&amp;#x27;, &amp;#x27;id&amp;#x27;: &amp;#x27;patient-0&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Patient&amp;#x27;}}...&amp;#x27;gender&amp;#x27;: &amp;#x27;male&amp;#x27;, &amp;#x27;id&amp;#x27;: &amp;#x27;patient-5&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Patient&amp;#x27;}}, ...], &amp;#x27;id&amp;#x27;: &amp;#x27;large-bundle&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Bundle&amp;#x27;}\n\n    def test_summarizer_patient_summary_performance(self, benchmark, large_patient_bundle):\n        &amp;quot;&amp;quot;&amp;quot;Benchmark patient summary generation performance.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        def run_summary():\n            return summarizer.create_patient_summary(large_patient_bundle)\n    \n&amp;gt;       result = benchmark(run_summary)\n\ntests/performance/test_performance.py:103: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:156: in __call__\n    return self._raw(function_to_benchmark, *args, **kwargs)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:180: in _raw\n    duration, iterations, loops_range = self._calibrate_timer(runner)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:318: in _calibrate_timer\n    duration = runner(loops_range)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:109: in runner\n    function_to_benchmark(*args, **kwargs)\ntests/performance/test_performance.py:101: in run_summary\n    return summarizer.create_patient_summary(large_patient_bundle)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10aed7b20&amp;gt;\npatient_data = {&amp;#x27;entry&amp;#x27;: [{&amp;#x27;resource&amp;#x27;: {&amp;#x27;birthDate&amp;#x27;: &amp;#x27;1900-01-01&amp;#x27;, &amp;#x27;gender&amp;#x27;: &amp;#x27;female&amp;#x27;, &amp;#x27;id&amp;#x27;: &amp;#x27;patient-0&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Patient&amp;#x27;}}...&amp;#x27;gender&amp;#x27;: &amp;#x27;male&amp;#x27;, &amp;#x27;id&amp;#x27;: &amp;#x27;patient-5&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Patient&amp;#x27;}}, ...], &amp;#x27;id&amp;#x27;: &amp;#x27;large-bundle&amp;#x27;, &amp;#x27;resourceType&amp;#x27;: &amp;#x27;Bundle&amp;#x27;}\ninclude_dates = True\n\n    def create_patient_summary(\n        self, patient_data: Dict[str, Any], include_dates: bool = None\n    ) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive patient summary optimized for NLP entity extraction and clinical trial matching.\n    \n        Args:\n            patient_data: A dictionary representing a patient&amp;#x27;s FHIR bundle.\n            include_dates: Whether to include dates in the summary. If None, uses instance default.\n    \n        Returns:\n            A natural language summary of the patient&amp;#x27;s mCODE data optimized for clinical trial matching.\n    \n        Raises:\n            ValueError: If the patient data is missing required fields.\n        &amp;quot;&amp;quot;&amp;quot;\n        if include_dates is None:\n            include_dates = self.include_dates\n        if not patient_data or &amp;quot;entry&amp;quot; not in patient_data:\n            raise ValueError(&amp;quot;Patient data is missing or not in the expected format.&amp;quot;)\n    \n        patient_resource = None\n        for entry in patient_data.get(&amp;quot;entry&amp;quot;, []):\n            if entry.get(&amp;quot;resource&amp;quot;, {}).get(&amp;quot;resourceType&amp;quot;) == &amp;quot;Patient&amp;quot;:\n                patient_resource = entry[&amp;quot;resource&amp;quot;]\n                break\n    \n        if not patient_resource:\n            raise ValueError(&amp;quot;No Patient resource found in the patient data.&amp;quot;)\n    \n        patient_id = patient_resource.get(&amp;quot;id&amp;quot;)\n        if not patient_id:\n            raise ValueError(&amp;quot;Patient resource is missing an &amp;#x27;id&amp;#x27;.&amp;quot;)\n    \n        name_data = patient_resource.get(&amp;quot;name&amp;quot;, [{}])[0]\n        full_name = f&amp;quot;{&amp;#x27; &amp;#x27;.join(name_data.get(&amp;#x27;given&amp;#x27;, []))} {name_data.get(&amp;#x27;family&amp;#x27;, &amp;#x27;&amp;#x27;)}&amp;quot;.strip()\n        gender = patient_resource.get(&amp;quot;gender&amp;quot;)\n        birth_date = patient_resource.get(&amp;quot;birthDate&amp;quot;)\n    \n        if not all([full_name, gender, birth_date]):\n&amp;gt;           raise ValueError(\n                &amp;quot;Patient resource is missing one or more of the following: name, gender, birthDate.&amp;quot;\n            )\nE           ValueError: Patient resource is missing one or more of the following: name, gender, birthDate.\n\nsrc/services/summarizer.py:759: ValueError\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_add_usage_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_add_usage_performance&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_add_usage_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_performance.TestPerformanceBenchmarks object at 0x10ad613f0&amp;gt;\nbenchmark = &amp;lt;pytest_benchmark.fixture.BenchmarkFixture object at 0x1096b62f0&amp;gt;\n\n    def test_token_tracker_add_usage_performance(self, benchmark):\n        &amp;quot;&amp;quot;&amp;quot;Benchmark token usage tracking performance.&amp;quot;&amp;quot;&amp;quot;\n        tracker = TokenTracker()\n    \n        def add_usage_batch():\n            for i in range(1000):\n                usage = TokenUsage(\n                    input_tokens=i * 10,\n                    output_tokens=i * 5,\n                    total_tokens=i * 15\n                )\n                tracker.add_usage(usage, f&amp;quot;component_{i % 10}&amp;quot;)\n    \n&amp;gt;       benchmark(add_usage_batch)\n\ntests/performance/test_performance.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:156: in __call__\n    return self._raw(function_to_benchmark, *args, **kwargs)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:180: in _raw\n    duration, iterations, loops_range = self._calibrate_timer(runner)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:318: in _calibrate_timer\n    duration = runner(loops_range)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:109: in runner\n    function_to_benchmark(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    def add_usage_batch():\n        for i in range(1000):\n&amp;gt;           usage = TokenUsage(\n                input_tokens=i * 10,\n                output_tokens=i * 5,\n                total_tokens=i * 15\n            )\nE           TypeError: TokenUsage.__init__() got an unexpected keyword argument &amp;#x27;input_tokens&amp;#x27;\n\ntests/performance/test_performance.py:114: TypeError\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_get_stats_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_get_stats_performance&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_token_tracker_get_stats_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_performance.TestPerformanceBenchmarks object at 0x10ad615a0&amp;gt;\nbenchmark = &amp;lt;pytest_benchmark.fixture.BenchmarkFixture object at 0x10b01eec0&amp;gt;\n\n    def test_token_tracker_get_stats_performance(self, benchmark):\n        &amp;quot;&amp;quot;&amp;quot;Benchmark getting token usage statistics.&amp;quot;&amp;quot;&amp;quot;\n        tracker = TokenTracker()\n    \n        # Pre-populate with data\n        for i in range(100):\n&amp;gt;           usage = TokenUsage(\n                input_tokens=i * 10,\n                output_tokens=i * 5,\n                total_tokens=i * 15\n            )\nE           TypeError: TokenUsage.__init__() got an unexpected keyword argument &amp;#x27;input_tokens&amp;#x27;\n\ntests/performance/test_performance.py:132: TypeError\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_api_cache_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Error&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_api_cache_performance::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_api_cache_performance::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;file /Users/idrdex/Documents/mcode_translator/tests/performance/test_performance.py, line 145\n      @patch(&amp;#x27;src.utils.api_manager.APICache._get_cache_path&amp;#x27;)\n      @patch(&amp;#x27;src.utils.api_manager.APICache._make_serializable&amp;#x27;)\n      def test_api_cache_performance(self, benchmark, mock_serializable, mock_cache_path):\nE       fixture &amp;#x27;mock_cache_path&amp;#x27; not found\n&amp;gt;       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, breast_cancer_patient, breast_cancer_trial, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, extra, extras, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, invalid_patient, invalid_trial, large_patient_bundle, large_trial, large_trial_data, lung_cancer_patient, lung_cancer_trial, mcode_factory, metadata, mock_api_response, mock_config, mock_core_memory_client, mock_llm_response, monkeypatch, no_cover, patient_factory, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient_data, sample_trial_data, temp_cache_dir, test_data_manager_fixture, test_logger, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, trial_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n&amp;gt;       use &amp;#x27;pytest --fixtures [testpath]&amp;#x27; for help on them.\n\n/Users/idrdex/Documents/mcode_translator/tests/performance/test_performance.py:145\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_json_processing_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_json_processing_performance&#34;, &#34;duration&#34;: &#34;611 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_json_processing_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;611 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_memory_usage_tracking&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_memory_usage_tracking&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_memory_usage_tracking&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_performance.TestPerformanceBenchmarks object at 0x10ad61ab0&amp;gt;\nbenchmark = &amp;lt;pytest_benchmark.fixture.BenchmarkFixture object at 0x10af96fe0&amp;gt;\n\n    def test_memory_usage_tracking(self, benchmark):\n        &amp;quot;&amp;quot;&amp;quot;Benchmark memory usage with large data structures.&amp;quot;&amp;quot;&amp;quot;\n        def create_large_structure():\n            # Create a large nested structure\n            return {\n                &amp;quot;trials&amp;quot;: [\n                    {\n                        &amp;quot;id&amp;quot;: f&amp;quot;trial_{i}&amp;quot;,\n                        &amp;quot;data&amp;quot;: &amp;quot;x&amp;quot; * 1000,\n                        &amp;quot;metadata&amp;quot;: {&amp;quot;size&amp;quot;: i} * 100\n                    } for i in range(100)\n                ]\n            }\n    \n&amp;gt;       result = benchmark(create_large_structure)\n\ntests/performance/test_performance.py:195: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:156: in __call__\n    return self._raw(function_to_benchmark, *args, **kwargs)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:180: in _raw\n    duration, iterations, loops_range = self._calibrate_timer(runner)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:318: in _calibrate_timer\n    duration = runner(loops_range)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/pytest_benchmark/fixture.py:109: in runner\n    function_to_benchmark(*args, **kwargs)\ntests/performance/test_performance.py:186: in create_large_structure\n    &amp;quot;trials&amp;quot;: [\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n.0 = &amp;lt;range_iterator object at 0x10af96490&amp;gt;\n\n        &amp;quot;trials&amp;quot;: [\n            {\n                &amp;quot;id&amp;quot;: f&amp;quot;trial_{i}&amp;quot;,\n                &amp;quot;data&amp;quot;: &amp;quot;x&amp;quot; * 1000,\n&amp;gt;               &amp;quot;metadata&amp;quot;: {&amp;quot;size&amp;quot;: i} * 100\n            } for i in range(100)\n        ]\n    }\nE   TypeError: unsupported operand type(s) for *: &amp;#x27;dict&amp;#x27; and &amp;#x27;int&amp;#x27;\n\ntests/performance/test_performance.py:190: TypeError\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_mcode_mapping_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Error&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_mcode_mapping_performance::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_mcode_mapping_performance::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;file /Users/idrdex/Documents/mcode_translator/tests/performance/test_performance.py, line 199\n      @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._validate_entities&amp;#x27;)\n      @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._call_llm_mapping&amp;#x27;)\n      @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._parse_llm_response&amp;#x27;)\n      def test_mcode_mapping_performance(self, benchmark, mock_parse, mock_call, mock_validate):\nE       fixture &amp;#x27;mock_validate&amp;#x27; not found\n&amp;gt;       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, breast_cancer_patient, breast_cancer_trial, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, extra, extras, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, invalid_patient, invalid_trial, large_patient_bundle, large_trial, large_trial_data, lung_cancer_patient, lung_cancer_trial, mcode_factory, metadata, mock_api_response, mock_config, mock_core_memory_client, mock_llm_response, monkeypatch, no_cover, patient_factory, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient_data, sample_trial_data, temp_cache_dir, test_data_manager_fixture, test_logger, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, trial_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n&amp;gt;       use &amp;#x27;pytest --fixtures [testpath]&amp;#x27; for help on them.\n\n/Users/idrdex/Documents/mcode_translator/tests/performance/test_performance.py:199\n&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_string_formatting_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_string_formatting_performance&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_string_formatting_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_list_comprehension_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_list_comprehension_performance&#34;, &#34;duration&#34;: &#34;845 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_list_comprehension_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;845 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_dict_operations_performance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_dict_operations_performance&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/performance/test_performance.py::TestPerformanceBenchmarks::test_dict_operations_performance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_workflow_base_class&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_workflow_base_class&#34;, &#34;duration&#34;: &#34;587 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_workflow_base_class&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;587 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_fetcher_workflows&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_fetcher_workflows&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_fetcher_workflows&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_processor_workflows&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_processor_workflows&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_processor_workflows&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_architecture.TestArchitecture object at 0x10add04f0&amp;gt;\n\n    def test_processor_workflows(self):\n        &amp;quot;&amp;quot;&amp;quot;Test that processor workflows can handle core memory storage.&amp;quot;&amp;quot;&amp;quot;\n        config = Config()\n    \n        # Mock memory storage\n        mock_memory = Mock()\n    \n        # Trials processor\n&amp;gt;       trials_processor = TrialsProcessorWorkflow(config, mock_memory)\nE       TypeError: Can&amp;#x27;t instantiate abstract class TrialsProcessorWorkflow with abstract method execute\n\ntests/test_architecture.py:62: TypeError\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_optimizer_workflow&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_optimizer_workflow&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_optimizer_workflow&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_architecture.TestArchitecture object at 0x10add06a0&amp;gt;\n\n    def test_optimizer_workflow(self):\n        &amp;quot;&amp;quot;&amp;quot;Test that optimizer workflow doesn&amp;#x27;t use core memory.&amp;quot;&amp;quot;&amp;quot;\n        config = Config()\n    \n&amp;gt;       optimizer = TrialsOptimizerWorkflow(config)\nE       TypeError: Can&amp;#x27;t instantiate abstract class TrialsOptimizerWorkflow with abstract method memory_space\n\ntests/test_architecture.py:75: TypeError\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_memory_storage&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_memory_storage&#34;, &#34;duration&#34;: &#34;141 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_memory_storage&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;141 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10aec0910&amp;gt;\npayload = {&amp;#x27;id&amp;#x27;: &amp;#x27;bca8bf82-89b5-49b7-9ca9-9f3e0bababf1&amp;#x27;, &amp;#x27;jsonrpc&amp;#x27;: &amp;#x27;2.0&amp;#x27;, &amp;#x27;method&amp;#x27;: &amp;#x27;initialize&amp;#x27;, &amp;#x27;params&amp;#x27;: {&amp;#x27;capabilities&amp;#x27;: {&amp;#x27;tools&amp;#x27;: True}, &amp;#x27;clientInfo&amp;#x27;: {&amp;#x27;name&amp;#x27;: &amp;#x27;Python-Script&amp;#x27;, &amp;#x27;version&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}, &amp;#x27;protocolVersion&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}}\nstream = False\n\n    def _post(self, payload: JSON, stream: bool = False) -&amp;gt; requests.Response:\n        &amp;quot;&amp;quot;&amp;quot;Send POST request to MCP server.&amp;quot;&amp;quot;&amp;quot;\n        response = requests.post(\n            self.url, json=payload, headers=self._headers(), timeout=60, stream=stream\n        )\n        try:\n&amp;gt;           response.raise_for_status()\n\nsrc/utils/core_memory_client.py:64: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;Response [401]&amp;gt;\n\n    def raise_for_status(self):\n        &amp;quot;&amp;quot;&amp;quot;Raises :class:`HTTPError`, if one occurred.&amp;quot;&amp;quot;&amp;quot;\n    \n        http_error_msg = &amp;quot;&amp;quot;\n        if isinstance(self.reason, bytes):\n            # We attempt to decode utf-8 first because some servers\n            # choose to localize their reason strings. If the string\n            # isn&amp;#x27;t utf-8, we fall back to iso-8859-1 for all other\n            # encodings. (See PR #3538)\n            try:\n                reason = self.reason.decode(&amp;quot;utf-8&amp;quot;)\n            except UnicodeDecodeError:\n                reason = self.reason.decode(&amp;quot;iso-8859-1&amp;quot;)\n        else:\n            reason = self.reason\n    \n        if 400 &amp;lt;= self.status_code &amp;lt; 500:\n            http_error_msg = (\n                f&amp;quot;{self.status_code} Client Error: {reason} for url: {self.url}&amp;quot;\n            )\n    \n        elif 500 &amp;lt;= self.status_code &amp;lt; 600:\n            http_error_msg = (\n                f&amp;quot;{self.status_code} Server Error: {reason} for url: {self.url}&amp;quot;\n            )\n    \n        if http_error_msg:\n&amp;gt;           raise HTTPError(http_error_msg, response=self)\nE           requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://core.heysol.ai/api/v1/mcp?source=test_source\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/requests/models.py:1026: HTTPError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10aec0910&amp;gt;\n\n    def _initialize_session(self) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize MCP session and list available tools.&amp;quot;&amp;quot;&amp;quot;\n        # Initialize session\n        init_payload = {\n            &amp;quot;jsonrpc&amp;quot;: &amp;quot;2.0&amp;quot;,\n            &amp;quot;id&amp;quot;: str(uuid.uuid4()),\n            &amp;quot;method&amp;quot;: &amp;quot;initialize&amp;quot;,\n            &amp;quot;params&amp;quot;: {\n                &amp;quot;protocolVersion&amp;quot;: &amp;quot;1.0.0&amp;quot;,\n                &amp;quot;capabilities&amp;quot;: {&amp;quot;tools&amp;quot;: True},\n                &amp;quot;clientInfo&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Python-Script&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;},\n            },\n        }\n    \n        try:\n&amp;gt;           response = self._post(init_payload)\n\nsrc/utils/core_memory_client.py:108: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10aec0910&amp;gt;\npayload = {&amp;#x27;id&amp;#x27;: &amp;#x27;bca8bf82-89b5-49b7-9ca9-9f3e0bababf1&amp;#x27;, &amp;#x27;jsonrpc&amp;#x27;: &amp;#x27;2.0&amp;#x27;, &amp;#x27;method&amp;#x27;: &amp;#x27;initialize&amp;#x27;, &amp;#x27;params&amp;#x27;: {&amp;#x27;capabilities&amp;#x27;: {&amp;#x27;tools&amp;#x27;: True}, &amp;#x27;clientInfo&amp;#x27;: {&amp;#x27;name&amp;#x27;: &amp;#x27;Python-Script&amp;#x27;, &amp;#x27;version&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}, &amp;#x27;protocolVersion&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}}\nstream = False\n\n    def _post(self, payload: JSON, stream: bool = False) -&amp;gt; requests.Response:\n        &amp;quot;&amp;quot;&amp;quot;Send POST request to MCP server.&amp;quot;&amp;quot;&amp;quot;\n        response = requests.post(\n            self.url, json=payload, headers=self._headers(), timeout=60, stream=stream\n        )\n        try:\n            response.raise_for_status()\n        except requests.HTTPError:\n&amp;gt;           raise CoreMemoryError(\n                f&amp;quot;HTTP error: {response.status_code} - {response.text}&amp;quot;\n            )\nE           src.utils.core_memory_client.CoreMemoryError: HTTP error: 401 - {&amp;quot;error&amp;quot;:&amp;quot;Authentication required&amp;quot;}\n\nsrc/utils/core_memory_client.py:66: CoreMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_architecture.TestArchitecture object at 0x10add0820&amp;gt;\nmock_client = &amp;lt;MagicMock name=&amp;#x27;CoreMemoryClient&amp;#x27; id=&amp;#x27;4478220416&amp;#x27;&amp;gt;\n\n    @patch(&amp;quot;src.utils.core_memory_client.CoreMemoryClient&amp;quot;)\n    def test_memory_storage(self, mock_client):\n        &amp;quot;&amp;quot;&amp;quot;Test mCODE memory storage interface.&amp;quot;&amp;quot;&amp;quot;\n        mock_client_instance = Mock()\n        mock_client.return_value = mock_client_instance\n    \n&amp;gt;       storage = McodeMemoryStorage(&amp;quot;test_key&amp;quot;, &amp;quot;test_source&amp;quot;)\n\ntests/test_architecture.py:87: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/storage/mcode_memory_storage.py:47: in __init__\n    self.client = CoreMemoryClient(\nsrc/utils/core_memory_client.py:45: in __init__\n    self._initialize_session()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10aec0910&amp;gt;\n\n    def _initialize_session(self) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize MCP session and list available tools.&amp;quot;&amp;quot;&amp;quot;\n        # Initialize session\n        init_payload = {\n            &amp;quot;jsonrpc&amp;quot;: &amp;quot;2.0&amp;quot;,\n            &amp;quot;id&amp;quot;: str(uuid.uuid4()),\n            &amp;quot;method&amp;quot;: &amp;quot;initialize&amp;quot;,\n            &amp;quot;params&amp;quot;: {\n                &amp;quot;protocolVersion&amp;quot;: &amp;quot;1.0.0&amp;quot;,\n                &amp;quot;capabilities&amp;quot;: {&amp;quot;tools&amp;quot;: True},\n                &amp;quot;clientInfo&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Python-Script&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;},\n            },\n        }\n    \n        try:\n            response = self._post(init_payload)\n            self.session_id = response.headers.get(&amp;quot;Mcp-Session-Id&amp;quot;) or self.session_id\n            self._parse_rpc(response)\n        except Exception as e:\n&amp;gt;           raise CoreMemoryError(f&amp;quot;Failed to initialize MCP session: {e}&amp;quot;)\nE           src.utils.core_memory_client.CoreMemoryError: Failed to initialize MCP session: HTTP error: 401 - {&amp;quot;error&amp;quot;:&amp;quot;Authentication required&amp;quot;}\n\nsrc/utils/core_memory_client.py:112: CoreMemoryError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=test_source HTTP/1.1&amp;quot; 401 35\n\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_cli_utils&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_cli_utils&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_cli_utils&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_workflow_inheritance&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_workflow_inheritance&#34;, &#34;duration&#34;: &#34;464 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_workflow_inheritance&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;464 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_architecture.TestArchitecture object at 0x10add0b80&amp;gt;\n\n    def test_workflow_inheritance(self):\n        &amp;quot;&amp;quot;&amp;quot;Test that workflows properly inherit from base classes.&amp;quot;&amp;quot;&amp;quot;\n        from src.workflows.base_workflow import (BaseWorkflow, FetcherWorkflow,\n                                                 ProcessorWorkflow)\n    \n        config = Config()\n    \n        # Test inheritance hierarchy\n        trials_fetcher = TrialsFetcherWorkflow(config)\n        assert isinstance(trials_fetcher, FetcherWorkflow)\n        assert isinstance(trials_fetcher, BaseWorkflow)\n    \n&amp;gt;       trials_processor = TrialsProcessorWorkflow(config)\nE       TypeError: Can&amp;#x27;t instantiate abstract class TrialsProcessorWorkflow with abstract method execute\n\ntests/test_architecture.py:121: TypeError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_workflow_error_handling&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_workflow_error_handling&#34;, &#34;duration&#34;: &#34;451 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_workflow_error_handling&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;451 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nERROR    TrialsFetcherWorkflow:base_workflow.py:184 test context: Test error\n\n&#34;}], &#34;tests/test_architecture.py::TestArchitecture::test_memory_storage_summaries&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_architecture.py::TestArchitecture::test_memory_storage_summaries&#34;, &#34;duration&#34;: &#34;237 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_architecture.py::TestArchitecture::test_memory_storage_summaries&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;237 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10af95090&amp;gt;\npayload = {&amp;#x27;id&amp;#x27;: &amp;#x27;65a2f0b3-9541-4cb1-a789-5827ed279738&amp;#x27;, &amp;#x27;jsonrpc&amp;#x27;: &amp;#x27;2.0&amp;#x27;, &amp;#x27;method&amp;#x27;: &amp;#x27;initialize&amp;#x27;, &amp;#x27;params&amp;#x27;: {&amp;#x27;capabilities&amp;#x27;: {&amp;#x27;tools&amp;#x27;: True}, &amp;#x27;clientInfo&amp;#x27;: {&amp;#x27;name&amp;#x27;: &amp;#x27;Python-Script&amp;#x27;, &amp;#x27;version&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}, &amp;#x27;protocolVersion&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}}\nstream = False\n\n    def _post(self, payload: JSON, stream: bool = False) -&amp;gt; requests.Response:\n        &amp;quot;&amp;quot;&amp;quot;Send POST request to MCP server.&amp;quot;&amp;quot;&amp;quot;\n        response = requests.post(\n            self.url, json=payload, headers=self._headers(), timeout=60, stream=stream\n        )\n        try:\n&amp;gt;           response.raise_for_status()\n\nsrc/utils/core_memory_client.py:64: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;Response [401]&amp;gt;\n\n    def raise_for_status(self):\n        &amp;quot;&amp;quot;&amp;quot;Raises :class:`HTTPError`, if one occurred.&amp;quot;&amp;quot;&amp;quot;\n    \n        http_error_msg = &amp;quot;&amp;quot;\n        if isinstance(self.reason, bytes):\n            # We attempt to decode utf-8 first because some servers\n            # choose to localize their reason strings. If the string\n            # isn&amp;#x27;t utf-8, we fall back to iso-8859-1 for all other\n            # encodings. (See PR #3538)\n            try:\n                reason = self.reason.decode(&amp;quot;utf-8&amp;quot;)\n            except UnicodeDecodeError:\n                reason = self.reason.decode(&amp;quot;iso-8859-1&amp;quot;)\n        else:\n            reason = self.reason\n    \n        if 400 &amp;lt;= self.status_code &amp;lt; 500:\n            http_error_msg = (\n                f&amp;quot;{self.status_code} Client Error: {reason} for url: {self.url}&amp;quot;\n            )\n    \n        elif 500 &amp;lt;= self.status_code &amp;lt; 600:\n            http_error_msg = (\n                f&amp;quot;{self.status_code} Server Error: {reason} for url: {self.url}&amp;quot;\n            )\n    \n        if http_error_msg:\n&amp;gt;           raise HTTPError(http_error_msg, response=self)\nE           requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://core.heysol.ai/api/v1/mcp?source=mcode_translator\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/site-packages/requests/models.py:1026: HTTPError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10af95090&amp;gt;\n\n    def _initialize_session(self) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize MCP session and list available tools.&amp;quot;&amp;quot;&amp;quot;\n        # Initialize session\n        init_payload = {\n            &amp;quot;jsonrpc&amp;quot;: &amp;quot;2.0&amp;quot;,\n            &amp;quot;id&amp;quot;: str(uuid.uuid4()),\n            &amp;quot;method&amp;quot;: &amp;quot;initialize&amp;quot;,\n            &amp;quot;params&amp;quot;: {\n                &amp;quot;protocolVersion&amp;quot;: &amp;quot;1.0.0&amp;quot;,\n                &amp;quot;capabilities&amp;quot;: {&amp;quot;tools&amp;quot;: True},\n                &amp;quot;clientInfo&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Python-Script&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;},\n            },\n        }\n    \n        try:\n&amp;gt;           response = self._post(init_payload)\n\nsrc/utils/core_memory_client.py:108: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10af95090&amp;gt;\npayload = {&amp;#x27;id&amp;#x27;: &amp;#x27;65a2f0b3-9541-4cb1-a789-5827ed279738&amp;#x27;, &amp;#x27;jsonrpc&amp;#x27;: &amp;#x27;2.0&amp;#x27;, &amp;#x27;method&amp;#x27;: &amp;#x27;initialize&amp;#x27;, &amp;#x27;params&amp;#x27;: {&amp;#x27;capabilities&amp;#x27;: {&amp;#x27;tools&amp;#x27;: True}, &amp;#x27;clientInfo&amp;#x27;: {&amp;#x27;name&amp;#x27;: &amp;#x27;Python-Script&amp;#x27;, &amp;#x27;version&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}, &amp;#x27;protocolVersion&amp;#x27;: &amp;#x27;1.0.0&amp;#x27;}}\nstream = False\n\n    def _post(self, payload: JSON, stream: bool = False) -&amp;gt; requests.Response:\n        &amp;quot;&amp;quot;&amp;quot;Send POST request to MCP server.&amp;quot;&amp;quot;&amp;quot;\n        response = requests.post(\n            self.url, json=payload, headers=self._headers(), timeout=60, stream=stream\n        )\n        try:\n            response.raise_for_status()\n        except requests.HTTPError:\n&amp;gt;           raise CoreMemoryError(\n                f&amp;quot;HTTP error: {response.status_code} - {response.text}&amp;quot;\n            )\nE           src.utils.core_memory_client.CoreMemoryError: HTTP error: 401 - {&amp;quot;error&amp;quot;:&amp;quot;Authentication required&amp;quot;}\n\nsrc/utils/core_memory_client.py:66: CoreMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_architecture.TestArchitecture object at 0x10add0280&amp;gt;\n\n    def test_memory_storage_summaries(self):\n        &amp;quot;&amp;quot;&amp;quot;Test that memory storage creates proper natural language summaries.&amp;quot;&amp;quot;&amp;quot;\n        with patch(&amp;quot;src.utils.core_memory_client.CoreMemoryClient&amp;quot;) as mock_client:\n            mock_client_instance = Mock()\n            mock_client.return_value = mock_client_instance\n    \n&amp;gt;           storage = McodeMemoryStorage(&amp;quot;test_key&amp;quot;)\n\ntests/test_architecture.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/storage/mcode_memory_storage.py:47: in __init__\n    self.client = CoreMemoryClient(\nsrc/utils/core_memory_client.py:45: in __init__\n    self._initialize_session()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.core_memory_client.CoreMemoryClient object at 0x10af95090&amp;gt;\n\n    def _initialize_session(self) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize MCP session and list available tools.&amp;quot;&amp;quot;&amp;quot;\n        # Initialize session\n        init_payload = {\n            &amp;quot;jsonrpc&amp;quot;: &amp;quot;2.0&amp;quot;,\n            &amp;quot;id&amp;quot;: str(uuid.uuid4()),\n            &amp;quot;method&amp;quot;: &amp;quot;initialize&amp;quot;,\n            &amp;quot;params&amp;quot;: {\n                &amp;quot;protocolVersion&amp;quot;: &amp;quot;1.0.0&amp;quot;,\n                &amp;quot;capabilities&amp;quot;: {&amp;quot;tools&amp;quot;: True},\n                &amp;quot;clientInfo&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;Python-Script&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.0.0&amp;quot;},\n            },\n        }\n    \n        try:\n            response = self._post(init_payload)\n            self.session_id = response.headers.get(&amp;quot;Mcp-Session-Id&amp;quot;) or self.session_id\n            self._parse_rpc(response)\n        except Exception as e:\n&amp;gt;           raise CoreMemoryError(f&amp;quot;Failed to initialize MCP session: {e}&amp;quot;)\nE           src.utils.core_memory_client.CoreMemoryError: Failed to initialize MCP session: HTTP error: 401 - {&amp;quot;error&amp;quot;:&amp;quot;Authentication required&amp;quot;}\n\nsrc/utils/core_memory_client.py:112: CoreMemoryError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 401 35\n\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_with_config&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_with_config&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_with_config&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_without_config&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_without_config&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_init_without_config&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_singleton&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_singleton&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_singleton&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_non_singleton&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_non_singleton&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_register_component_non_singleton&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_dependency_container.TestDependencyContainer object at 0x10add2710&amp;gt;\n\n    def test_register_component_non_singleton(self):\n        &amp;quot;&amp;quot;&amp;quot;Test registering a non-singleton component.&amp;quot;&amp;quot;&amp;quot;\n        container = DependencyContainer()\n        component_factory = Mock(return_value=Mock())\n    \n        container.register_component(&amp;quot;test&amp;quot;, component_factory, singleton=False)\n    \n        comp1 = container.get_component(&amp;quot;test&amp;quot;)\n        comp2 = container.get_component(&amp;quot;test&amp;quot;)\n    \n&amp;gt;       assert comp1 != comp2\nE       AssertionError: assert &amp;lt;Mock id=&amp;#x27;4486538304&amp;#x27;&amp;gt; != &amp;lt;Mock id=&amp;#x27;4486538304&amp;#x27;&amp;gt;\n\ntests/unit/test_dependency_container.py:63: AssertionError\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_get_component_not_found&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_get_component_not_found&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_get_component_not_found&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_dependency_container.TestDependencyContainer object at 0x10add28f0&amp;gt;\n\n    def test_get_component_not_found(self):\n        &amp;quot;&amp;quot;&amp;quot;Test getting non-existent component raises error.&amp;quot;&amp;quot;&amp;quot;\n        container = DependencyContainer()\n    \n&amp;gt;       with pytest.raises(KeyError):\nE       Failed: DID NOT RAISE &amp;lt;class &amp;#x27;KeyError&amp;#x27;&amp;gt;\n\ntests/unit/test_dependency_container.py:70: Failed\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_memory_storage&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_memory_storage&#34;, &#34;duration&#34;: &#34;465 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_memory_storage&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;465 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_dependency_container.TestDependencyContainer object at 0x10add2aa0&amp;gt;\nmock_config_class = &amp;lt;MagicMock name=&amp;#x27;Config&amp;#x27; id=&amp;#x27;4452999104&amp;#x27;&amp;gt;\nmock_storage_class = &amp;lt;MagicMock name=&amp;#x27;DataStorage&amp;#x27; id=&amp;#x27;4478206448&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.core.dependency_container.DataStorage&amp;#x27;)\n    @patch(&amp;#x27;src.core.dependency_container.Config&amp;#x27;)\n    def test_create_memory_storage(self, mock_config_class, mock_storage_class):\n        &amp;quot;&amp;quot;&amp;quot;Test creating memory storage with mock.&amp;quot;&amp;quot;&amp;quot;\n        mock_config = Mock()\n        mock_config_class.return_value = mock_config\n        mock_storage = Mock()\n        mock_storage_class.return_value = mock_storage\n    \n        container = DependencyContainer(mock_config)\n        result = container.create_memory_storage()\n    \n&amp;gt;       assert result == mock_storage\nE       AssertionError: assert &amp;lt;src.storage.mcode_memory_storage.McodeMemoryStorage object at 0x10b6b2500&amp;gt; == &amp;lt;Mock name=&amp;#x27;DataStorage()&amp;#x27; id=&amp;#x27;4486538256&amp;#x27;&amp;gt;\n\ntests/unit/test_dependency_container.py:85: AssertionError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_clinical_trial_pipeline&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_clinical_trial_pipeline&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_clinical_trial_pipeline&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;args = (&amp;lt;test_dependency_container.TestDependencyContainer object at 0x10add2c50&amp;gt;,)\nkeywargs = {}\n\n    @wraps(func)\n    def patched(*args, **keywargs):\n&amp;gt;       with self.decoration_helper(patched,\n                                    args,\n                                    keywargs) as (newargs, newkeywargs):\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1376: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/contextlib.py:135: in __enter__\n    return next(self.gen)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1358: in decoration_helper\n    arg = exit_stack.enter_context(patching)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/contextlib.py:492: in enter_context\n    result = _cm_type.__enter__(cm)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1447: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;unittest.mock._patch object at 0x10add1930&amp;gt;\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n&amp;gt;           raise AttributeError(\n                &amp;quot;%s does not have the attribute %r&amp;quot; % (target, name)\n            )\nE           AttributeError: &amp;lt;module &amp;#x27;src.core.dependency_container&amp;#x27; from &amp;#x27;/Users/idrdex/Documents/mcode_translator/src/core/dependency_container.py&amp;#x27;&amp;gt; does not have the attribute &amp;#x27;ClinicalTrialPipeline&amp;#x27;\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1420: AttributeError\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_patient_data_pipeline&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_patient_data_pipeline&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestDependencyContainer::test_create_patient_data_pipeline&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;args = (&amp;lt;test_dependency_container.TestDependencyContainer object at 0x10add2e00&amp;gt;,)\nkeywargs = {}\n\n    @wraps(func)\n    def patched(*args, **keywargs):\n&amp;gt;       with self.decoration_helper(patched,\n                                    args,\n                                    keywargs) as (newargs, newkeywargs):\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1376: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/contextlib.py:135: in __enter__\n    return next(self.gen)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1358: in decoration_helper\n    arg = exit_stack.enter_context(patching)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/contextlib.py:492: in enter_context\n    result = _cm_type.__enter__(cm)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1447: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;unittest.mock._patch object at 0x10add1ae0&amp;gt;\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n&amp;gt;           raise AttributeError(\n                &amp;quot;%s does not have the attribute %r&amp;quot; % (target, name)\n            )\nE           AttributeError: &amp;lt;module &amp;#x27;src.core.dependency_container&amp;#x27; from &amp;#x27;/Users/idrdex/Documents/mcode_translator/src/core/dependency_container.py&amp;#x27;&amp;gt; does not have the attribute &amp;#x27;PatientDataPipeline&amp;#x27;\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:1420: AttributeError\n&#34;}], &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_get_container_creates_default&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_get_container_creates_default&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_get_container_creates_default&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_set_container&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_set_container&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_set_container&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_reset_container&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_reset_container&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_dependency_container.py::TestGlobalContainerFunctions::test_reset_container&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_logger_fixture&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_logger_fixture&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_logger_fixture&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_debug_logging&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_debug_logging&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_debug_logging&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    test_logger:test_logging.py:18 This is a DEBUG message for testing color logging\n\n&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_info_logging&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_info_logging&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_info_logging&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nINFO     test_logger:test_logging.py:23 This is an INFO message for testing color logging\n\n&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_warning_logging&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_warning_logging&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_warning_logging&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nWARNING  test_logger:test_logging.py:28 This is a WARNING message for testing color logging\n\n&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_error_logging&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_error_logging&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_error_logging&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nERROR    test_logger:test_logging.py:33 This is an ERROR message for testing color logging\n\n&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_critical_logging&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_critical_logging&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_critical_logging&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nCRITICAL test_logger:test_logging.py:38 This is a CRITICAL message for testing color logging\n\n&#34;}], &#34;tests/unit/test_logging.py::TestColorLogging::test_multiple_log_levels&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_logging.py::TestColorLogging::test_multiple_log_levels&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_logging.py::TestColorLogging::test_multiple_log_levels&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    test_logger:test_logging.py:43 Debug: Starting test\nINFO     test_logger:test_logging.py:44 Info: Processing data\nWARNING  test_logger:test_logging.py:45 Warning: Something might be wrong\nERROR    test_logger:test_logging.py:46 Error: An error occurred\nINFO     test_logger:test_logging.py:47 Info: Test completed\n\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_init&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_init&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_init&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10aed6260&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adf9c30&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4478295472&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4477728224&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_init(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test McodeMapper initialization.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10aed6260&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_success&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_success&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_success&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af07310&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adf9db0&amp;gt;\nmock_parse = &amp;lt;MagicMock name=&amp;#x27;_parse_llm_response&amp;#x27; id=&amp;#x27;4478498176&amp;#x27;&amp;gt;\nmock_call_llm = &amp;lt;MagicMock name=&amp;#x27;_call_llm_mapping&amp;#x27; id=&amp;#x27;4478500672&amp;#x27;&amp;gt;\nmock_validate = &amp;lt;MagicMock name=&amp;#x27;_validate_entities&amp;#x27; id=&amp;#x27;4479077872&amp;#x27;&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479010272&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479019008&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._validate_entities&amp;#x27;)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._call_llm_mapping&amp;#x27;)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._parse_llm_response&amp;#x27;)\n    def test_map_to_mcode_success(self, mock_parse, mock_call_llm, mock_validate,\n                                  mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test successful mCODE mapping.&amp;quot;&amp;quot;&amp;quot;\n        # Setup mocks\n        mock_validate.return_value = None\n        mock_call_llm.return_value = {&amp;quot;response&amp;quot;: &amp;quot;mock response&amp;quot;}\n        mock_parse.return_value = {\n            &amp;quot;mcode_elements&amp;quot;: [{&amp;quot;element&amp;quot;: &amp;quot;TestElement&amp;quot;}],\n            &amp;quot;metadata&amp;quot;: {&amp;quot;confidence&amp;quot;: 0.9}\n        }\n    \n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af07310&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_validation_error&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_validation_error&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_map_to_mcode_validation_error&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f802fb0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adf9f30&amp;gt;\nmock_validate = &amp;lt;MagicMock name=&amp;#x27;_validate_entities&amp;#x27; id=&amp;#x27;4555022368&amp;#x27;&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555029376&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555222576&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._validate_entities&amp;#x27;)\n    def test_map_to_mcode_validation_error(self, mock_validate,\n                                          mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test mCODE mapping with validation error.&amp;quot;&amp;quot;&amp;quot;\n        mock_validate.side_effect = ValueError(&amp;quot;Invalid input&amp;quot;)\n    \n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:56: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f802fb0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_valid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_valid&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_valid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x1096b7310&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa0b0&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4452998000&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4478497648&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_entities_valid(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test entity validation with valid input.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:65: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x1096b7310&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_empty&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_empty&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_empty&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f870af0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa260&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555479488&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479736496&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_entities_empty(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test entity validation with empty input.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:74: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f870af0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_none&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_none&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_entities_none&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10b01c3d0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa410&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479645056&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4486535856&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_entities_none(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test entity validation with None input.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10b01c3d0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_call_llm_mapping&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_call_llm_mapping&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_call_llm_mapping&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f8013c0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa5c0&amp;gt;\nmock_call_llm = &amp;lt;MagicMock name=&amp;#x27;_call_llm_api&amp;#x27; id=&amp;#x27;4555019440&amp;#x27;&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555016656&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4477726688&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper._call_llm_api&amp;#x27;)\n    def test_call_llm_mapping(self, mock_call_llm, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test LLM mapping call.&amp;quot;&amp;quot;&amp;quot;\n        mock_call_llm.return_value = &amp;quot;mock response&amp;quot;\n    \n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f8013c0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_valid_json&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_valid_json&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_valid_json&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10aeb5fc0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa770&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4478160944&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4478172320&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_parse_llm_response_valid_json(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test parsing valid JSON response.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:105: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10aeb5fc0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_invalid_json&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_invalid_json&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_parse_llm_response_invalid_json&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10b01c7c0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfa920&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479642032&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479644288&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_parse_llm_response_invalid_json(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test parsing invalid JSON response.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:117: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10b01c7c0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_valid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_valid&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_valid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10ad17940&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfaad0&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4476467280&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479732944&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_mapping_response_structure_valid(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test validating valid mapping response structure.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:127: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10ad17940&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_missing_elements&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_missing_elements&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mapping_response_structure_missing_elements&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af84df0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfac80&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479024864&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479014016&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_mapping_response_structure_missing_elements(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test validating response missing mcode_elements.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:147: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af84df0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_valid&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_valid&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_valid&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10adfa650&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfae30&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4477397472&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555486592&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_mcode_element_strict_valid(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test validating valid mCODE element.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:157: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10adfa650&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_missing_fields&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_missing_fields&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_validate_mcode_element_strict_missing_fields&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f830df0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfafe0&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555226896&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4555223824&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    def test_validate_mcode_element_strict_missing_fields(self, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test validating mCODE element with missing fields.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:172: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10f830df0&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_process_request&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_process_request&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_mcode_llm.py::TestMcodeMapper::test_process_request&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af95060&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n&amp;gt;               raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\nE               src.pipeline.mcode_llm.McodeConfigurationError: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:149: McodeConfigurationError\n\nDuring handling of the above exception, another exception occurred:\n\nself = &amp;lt;test_mcode_llm.TestMcodeMapper object at 0x10adfb190&amp;gt;\nmock_map = &amp;lt;MagicMock name=&amp;#x27;map_to_mcode&amp;#x27; id=&amp;#x27;4479077008&amp;#x27;&amp;gt;\nmock_loggable_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4479077728&amp;#x27;&amp;gt;\nmock_llm_init = &amp;lt;MagicMock name=&amp;#x27;__init__&amp;#x27; id=&amp;#x27;4478537008&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.pipeline.mcode_llm.LlmBase.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.Loggable.__init__&amp;#x27;, return_value=None)\n    @patch(&amp;#x27;src.pipeline.mcode_llm.McodeMapper.map_to_mcode&amp;#x27;)\n    def test_process_request(self, mock_map, mock_loggable_init, mock_llm_init):\n        &amp;quot;&amp;quot;&amp;quot;Test processing request.&amp;quot;&amp;quot;&amp;quot;\n        mock_map.return_value = {&amp;quot;result&amp;quot;: &amp;quot;test&amp;quot;}\n    \n&amp;gt;       mapper = McodeMapper()\n\ntests/unit/test_mcode_llm.py:185: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.pipeline.mcode_llm.McodeMapper object at 0x10af95060&amp;gt;\nprompt_name = &amp;#x27;direct_mcode_evidence_based_concise&amp;#x27;, model_name = None\ntemperature = None, max_tokens = None\n\n    def __init__(\n        self,\n        prompt_name: str = &amp;quot;direct_mcode_evidence_based_concise&amp;quot;,\n        model_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Initialize the mCODE mapper with specified configuration.\n    \n        Args:\n            prompt_name: Name of the prompt template to use for mapping.\n                Defaults to evidence-based concise prompt for optimal quality.\n            model_name: LLM model name. If None, uses configuration default.\n            temperature: Temperature for text generation. If None, uses config default.\n            max_tokens: Maximum tokens for response. If None, uses config default.\n    \n        Raises:\n            McodeConfigurationError: If configuration validation fails.\n            LlmConfigurationError: If LLM initialization fails.\n        &amp;quot;&amp;quot;&amp;quot;\n        try:\n            # STRICT: No fallback to default configuration - all parameters must be explicitly provided\n            if not model_name:\n                raise McodeConfigurationError(\n                    &amp;quot;Model name is required - no fallback to default model allowed in strict mode&amp;quot;\n                )\n    \n            # Get configuration for the specified model only\n            config = Config()\n            final_model_name = model_name\n            final_temperature = (\n                temperature\n                if temperature is not None\n                else config.get_temperature(model_name)\n            )\n            final_max_tokens = (\n                max_tokens\n                if max_tokens is not None\n                else config.get_max_tokens(model_name)\n            )\n    \n            # Initialize LlmBase with explicit configuration\n            LlmBase.__init__(\n                self,\n                model_name=final_model_name,\n                temperature=final_temperature,\n                max_tokens=final_max_tokens,\n                response_format={&amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot;},\n            )\n    \n            # Initialize Loggable\n            Loggable.__init__(self)\n    \n            # Standard mCODE value sets\n            self.mcode_value_sets = {\n                &amp;quot;gender&amp;quot;: [&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;other&amp;quot;, &amp;quot;unknown&amp;quot;],\n                &amp;quot;ethnicity&amp;quot;: [\n                    &amp;quot;hispanic-or-latino&amp;quot;,\n                    &amp;quot;not-hispanic-or-latino&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n                &amp;quot;race&amp;quot;: [\n                    &amp;quot;american-indian-or-alaska-native&amp;quot;,\n                    &amp;quot;asian&amp;quot;,\n                    &amp;quot;black-or-african-american&amp;quot;,\n                    &amp;quot;native-hawaiian-or-other-pacific-islander&amp;quot;,\n                    &amp;quot;white&amp;quot;,\n                    &amp;quot;other&amp;quot;,\n                    &amp;quot;unknown&amp;quot;,\n                ],\n            }\n    \n            # Initialize prompt loader - don&amp;#x27;t load prompt template here\n            # Prompt template will be set by pipeline or via prompt_key parameter\n            self.prompt_loader = PromptLoader()\n            self.prompt_name = prompt_name\n            self.MCODE_MAPPING_PROMPT_TEMPLATE = self.prompt_loader.get_prompt(\n                self.prompt_name\n            )\n    \n        except Exception as e:\n&amp;gt;           raise McodeConfigurationError(f&amp;quot;Failed to initialize McodeMapper: {str(e)}&amp;quot;)\nE           src.pipeline.mcode_llm.McodeConfigurationError: Failed to initialize McodeMapper: Model name is required - no fallback to default model allowed in strict mode\n\nsrc/pipeline/mcode_llm.py:207: McodeConfigurationError\n&#34;}], &#34;tests/unit/test_models.py::TestClinicalTrialData::test_valid_clinical_trial_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestClinicalTrialData::test_valid_clinical_trial_data&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestClinicalTrialData::test_valid_clinical_trial_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestClinicalTrialData::test_invalid_clinical_trial_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestClinicalTrialData::test_invalid_clinical_trial_data&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestClinicalTrialData::test_invalid_clinical_trial_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestMcodeElement::test_valid_mcode_element&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestMcodeElement::test_valid_mcode_element&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestMcodeElement::test_valid_mcode_element&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestMcodeElement::test_invalid_confidence_score&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestMcodeElement::test_invalid_confidence_score&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestMcodeElement::test_invalid_confidence_score&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestPipelineResult::test_valid_pipeline_result&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestPipelineResult::test_valid_pipeline_result&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestPipelineResult::test_valid_pipeline_result&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestWorkflowResult::test_valid_workflow_result&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestWorkflowResult::test_valid_workflow_result&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestWorkflowResult::test_valid_workflow_result&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestBenchmarkResult::test_valid_benchmark_result&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestBenchmarkResult::test_valid_benchmark_result&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestBenchmarkResult::test_valid_benchmark_result&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestPatientData::test_valid_patient_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestPatientData::test_valid_patient_data&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestPatientData::test_valid_patient_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestPatientData::test_invalid_patient_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestPatientData::test_invalid_patient_data&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestPatientData::test_invalid_patient_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestTokenUsage::test_token_usage_with_total&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestTokenUsage::test_token_usage_with_total&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestTokenUsage::test_token_usage_with_total&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestTokenUsage::test_token_usage_calculate_total&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestTokenUsage::test_token_usage_calculate_total&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestTokenUsage::test_token_usage_calculate_total&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_models.py::TestValidationResult::test_validation_result&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_models.py::TestValidationResult::test_validation_result&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_models.py::TestValidationResult::test_validation_result&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_basic_loading&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_basic_loading&#34;, &#34;duration&#34;: &#34;9 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_basic_loading&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;9 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;create_test_zip = &amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_basic_l0/test_patients.zip&amp;#x27;\n\n    def test_patient_generator_basic_loading(create_test_zip):\n        &amp;quot;&amp;quot;&amp;quot;Test basic PatientGenerator loading from ZIP archive.&amp;quot;&amp;quot;&amp;quot;\n        generator = PatientGenerator(create_test_zip)\n    \n        assert (\n            len(generator) == 4\n        )  # 1 bundle + 1 single patient + 2 from NDJSON + 1 invalid file that gets parsed\n&amp;gt;       assert len(generator.get_patient_ids()) &amp;gt;= 3  # At least 3 unique patient IDs\n\ntests/unit/test_patient_generator.py:102: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/utils/patient_generator.py:487: in get_patient_ids\n    for patient in self:\nsrc/utils/patient_generator.py:351: in __next__\n    patient = self._load_patient_from_file(fname)\nsrc/utils/patient_generator.py:366: in _load_patient_from_file\n    data = json.loads(content)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;json.decoder.JSONDecoder object at 0x10800bb80&amp;gt;\ns = &amp;#x27;{&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;collection&amp;quot;, &amp;quot;entry&amp;quot;: [{&amp;quot;resource&amp;quot;: {&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;patient-12...value&amp;quot;: &amp;quot;PT123&amp;quot;}], &amp;quot;name&amp;quot;: [{&amp;quot;family&amp;quot;: &amp;quot;Johnson&amp;quot;, &amp;quot;given&amp;quot;: [&amp;quot;John&amp;quot;]}], &amp;quot;gender&amp;quot;: &amp;quot;male&amp;quot;, &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;}}]}&amp;#x27;\n_w = &amp;lt;built-in method match of re.Pattern object at 0x107da7370&amp;gt;\n\n    def decode(self, s, _w=WHITESPACE.match):\n        &amp;quot;&amp;quot;&amp;quot;Return the Python representation of ``s`` (a ``str`` instance\n        containing a JSON document).\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n        end = _w(s, end).end()\n        if end != len(s):\n&amp;gt;           raise JSONDecodeError(&amp;quot;Extra data&amp;quot;, s, end)\nE           json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 253)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/decoder.py:340: JSONDecodeError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_basic_l0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nWARNING  src.utils.patient_generator:patient_generator.py:479 get_patient_ids() requires loading all patients - this may be slow\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_random_selection&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_random_selection&#34;, &#34;duration&#34;: &#34;6 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_random_selection&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;6 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_random_0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_specific_id&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_specific_id&#34;, &#34;duration&#34;: &#34;10 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_specific_id&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;10 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;create_test_zip = &amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_specifi0/test_patients.zip&amp;#x27;\n\n    def test_patient_generator_specific_id(create_test_zip):\n        &amp;quot;&amp;quot;&amp;quot;Test getting patient by specific ID.&amp;quot;&amp;quot;&amp;quot;\n        generator = PatientGenerator(create_test_zip)\n    \n        # Test with known ID from fixture\n        patient = generator.get_patient_by_id(&amp;quot;patient-123&amp;quot;)\n        assert patient.get(&amp;quot;resourceType&amp;quot;) == &amp;quot;Bundle&amp;quot;\n    \n        # Test with identifier value\n&amp;gt;       patient_by_id = generator.get_patient_by_id(&amp;quot;PT123&amp;quot;)\n\ntests/unit/test_patient_generator.py:140: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.patient_generator.PatientGenerator object at 0x10b036d40&amp;gt;\npatient_id = &amp;#x27;PT123&amp;#x27;\n\n    def get_patient_by_id(self, patient_id: str) -&amp;gt; Dict[str, Any]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get specific patient by ID.\n    \n        Args:\n            patient_id: Patient identifier\n    \n        Returns:\n            Patient bundle matching the ID\n    \n        Raises:\n            PatientNotFoundError: If patient ID not found\n        &amp;quot;&amp;quot;&amp;quot;\n        # Search through all patient files (this is expensive for large archives)\n        self.logger.warning(\n            f&amp;quot;Searching for patient {patient_id} - this may be slow for large archives&amp;quot;\n        )\n    \n        for fname in self._patient_files:\n            try:\n                patient = self._load_patient_from_file(fname)\n                if self._extract_patient_id(patient) == patient_id:\n                    return patient\n            except Exception as e:\n                self.logger.debug(f&amp;quot;Error loading {fname}: {e}&amp;quot;)\n                continue\n    \n&amp;gt;       raise PatientNotFoundError(\n            f&amp;quot;Patient with ID &amp;#x27;{patient_id}&amp;#x27; not found in archive&amp;quot;\n        )\nE       src.utils.patient_generator.PatientNotFoundError: Patient with ID &amp;#x27;PT123&amp;#x27; not found in archive\n\nsrc/utils/patient_generator.py:445: PatientNotFoundError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_specifi0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nWARNING  src.utils.patient_generator:patient_generator.py:432 Searching for patient patient-123 - this may be slow for large archives\nWARNING  src.utils.patient_generator:patient_generator.py:432 Searching for patient PT123 - this may be slow for large archives\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\nDEBUG    src.utils.patient_generator:patient_generator.py:442 Error loading patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/invalid.txt: Expecting value: line 1 column 1 (char 0)\nDEBUG    src.utils.patient_generator:patient_generator.py:442 Error loading patients/invalid.txt: Expecting value: line 1 column 1 (char 0)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_exclude_ids&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_exclude_ids&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_exclude_ids&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;create_test_zip = &amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_exclude0/test_patients.zip&amp;#x27;\n\n    def test_patient_generator_exclude_ids(create_test_zip):\n        &amp;quot;&amp;quot;&amp;quot;Test random selection with excluded IDs.&amp;quot;&amp;quot;&amp;quot;\n        generator = PatientGenerator(create_test_zip)\n    \n        # Exclude one patient, should still get a different one\n&amp;gt;       random_patient = generator.get_random_patient(exclude_ids=[&amp;quot;patient-123&amp;quot;])\n\ntests/unit/test_patient_generator.py:153: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/utils/patient_generator.py:408: in get_random_patient\n    patient = self._load_patient_from_file(fname)\nsrc/utils/patient_generator.py:366: in _load_patient_from_file\n    data = json.loads(content)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;json.decoder.JSONDecoder object at 0x10800bb80&amp;gt;\ns = &amp;#x27;{&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;collection&amp;quot;, &amp;quot;entry&amp;quot;: [{&amp;quot;resource&amp;quot;: {&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;patient-12...value&amp;quot;: &amp;quot;PT123&amp;quot;}], &amp;quot;name&amp;quot;: [{&amp;quot;family&amp;quot;: &amp;quot;Johnson&amp;quot;, &amp;quot;given&amp;quot;: [&amp;quot;John&amp;quot;]}], &amp;quot;gender&amp;quot;: &amp;quot;male&amp;quot;, &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;}}]}&amp;#x27;\n_w = &amp;lt;built-in method match of re.Pattern object at 0x107da7370&amp;gt;\n\n    def decode(self, s, _w=WHITESPACE.match):\n        &amp;quot;&amp;quot;&amp;quot;Return the Python representation of ``s`` (a ``str`` instance\n        containing a JSON document).\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n        end = _w(s, end).end()\n        if end != len(s):\n&amp;gt;           raise JSONDecodeError(&amp;quot;Extra data&amp;quot;, s, end)\nE           json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 253)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/decoder.py:340: JSONDecodeError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_exclude0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_shuffle&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_shuffle&#34;, &#34;duration&#34;: &#34;7 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_shuffle&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;7 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;create_test_zip = &amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_shuffle0/test_patients.zip&amp;#x27;\n\n    def test_patient_generator_shuffle(create_test_zip):\n        &amp;quot;&amp;quot;&amp;quot;Test patient shuffling with seed for reproducibility.&amp;quot;&amp;quot;&amp;quot;\n        generator1 = PatientGenerator(create_test_zip, shuffle=True, seed=42)\n        generator2 = PatientGenerator(create_test_zip, shuffle=True, seed=42)\n    \n        # Both generators should produce the same shuffled order\n&amp;gt;       patients1 = list(generator1)\n\ntests/unit/test_patient_generator.py:173: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/utils/patient_generator.py:351: in __next__\n    patient = self._load_patient_from_file(fname)\nsrc/utils/patient_generator.py:366: in _load_patient_from_file\n    data = json.loads(content)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;json.decoder.JSONDecoder object at 0x10800bb80&amp;gt;\ns = &amp;#x27;{&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;collection&amp;quot;, &amp;quot;entry&amp;quot;: [{&amp;quot;resource&amp;quot;: {&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;patient-12...value&amp;quot;: &amp;quot;PT123&amp;quot;}], &amp;quot;name&amp;quot;: [{&amp;quot;family&amp;quot;: &amp;quot;Johnson&amp;quot;, &amp;quot;given&amp;quot;: [&amp;quot;John&amp;quot;]}], &amp;quot;gender&amp;quot;: &amp;quot;male&amp;quot;, &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;}}]}&amp;#x27;\n_w = &amp;lt;built-in method match of re.Pattern object at 0x107da7370&amp;gt;\n\n    def decode(self, s, _w=WHITESPACE.match):\n        &amp;quot;&amp;quot;&amp;quot;Return the Python representation of ``s`` (a ``str`` instance\n        containing a JSON document).\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n        end = _w(s, end).end()\n        if end != len(s):\n&amp;gt;           raise JSONDecodeError(&amp;quot;Extra data&amp;quot;, s, end)\nE           json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 253)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/decoder.py:340: JSONDecodeError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    src.utils.patient_generator:patient_generator.py:87 Random seed set to 42 for reproducible shuffling\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_shuffle0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nDEBUG    src.utils.patient_generator:patient_generator.py:87 Random seed set to 42 for reproducible shuffling\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_shuffle0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_limit_and_start&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_limit_and_start&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_limit_and_start&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_limit_a0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_create_patient_generator_with_config_name&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_create_patient_generator_with_config_name&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_create_patient_generator_with_config_name&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_create_patient_generator_with_config_name():\n        &amp;quot;&amp;quot;&amp;quot;Test factory function with configuration-based archive name.&amp;quot;&amp;quot;&amp;quot;\n        with patch(&amp;quot;src.utils.config.Config&amp;quot;) as mock_config:\n            # Mock config to return a valid archive path\n            mock_config_instance = MagicMock()\n            mock_config.return_value = mock_config_instance\n            mock_config_instance.config_data = {\n                &amp;quot;synthetic_data&amp;quot;: {\n                    &amp;quot;base_directory&amp;quot;: &amp;quot;data/synthetic_patients&amp;quot;,\n                    &amp;quot;archives&amp;quot;: {&amp;quot;breast_cancer&amp;quot;: {&amp;quot;10_years&amp;quot;: {&amp;quot;url&amp;quot;: &amp;quot;test_url&amp;quot;}}},\n                }\n            }\n    \n            # Mock os.path.exists to return True for resolved path\n            with patch(&amp;quot;os.path.exists&amp;quot;) as mock_exists, patch(\n                &amp;quot;src.utils.patient_generator.PatientGenerator&amp;quot;\n            ) as mock_generator:\n    \n                mock_exists.return_value = True\n                create_patient_generator(&amp;quot;breast_cancer/10_years&amp;quot;)\n    \n                # Verify PatientGenerator was called with resolved path\n                expected_path = os.path.join(\n                    &amp;quot;data/synthetic_patients&amp;quot;,\n                    &amp;quot;breast_cancer&amp;quot;,\n                    &amp;quot;10_years&amp;quot;,\n                    &amp;quot;breast_cancer_10_years.zip&amp;quot;,\n                )\n&amp;gt;               mock_generator.assert_called_once_with(\n                    expected_path, mock_config_instance, False, None\n                )\n\ntests/unit/test_patient_generator.py:224: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:941: in assert_called_once_with\n    return self.assert_called_with(*args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;MagicMock name=&amp;#x27;PatientGenerator&amp;#x27; id=&amp;#x27;4555538240&amp;#x27;&amp;gt;\nargs = (&amp;#x27;data/synthetic_patients/breast_cancer/10_years/breast_cancer_10_years.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4479757056&amp;#x27;&amp;gt;, False, None)\nkwargs = {}\nexpected = call(&amp;#x27;data/synthetic_patients/breast_cancer/10_years/breast_cancer_10_years.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4479757056&amp;#x27;&amp;gt;, False, None)\nactual = call(&amp;#x27;breast_cancer/10_years&amp;#x27;, &amp;lt;src.utils.config.Config object at 0x10f87dcf0&amp;gt;, False, None)\n_error_message = &amp;lt;function NonCallableMock.assert_called_with.&amp;lt;locals&amp;gt;._error_message at 0x10b010160&amp;gt;\ncause = None\n\n    def assert_called_with(self, /, *args, **kwargs):\n        &amp;quot;&amp;quot;&amp;quot;assert that the last call was made with the specified arguments.\n    \n        Raises an AssertionError if the args and keyword args passed in are\n        different to the last call to the mock.&amp;quot;&amp;quot;&amp;quot;\n        if self.call_args is None:\n            expected = self._format_mock_call_signature(args, kwargs)\n            actual = &amp;#x27;not called.&amp;#x27;\n            error_message = (&amp;#x27;expected call not found.\\nExpected: %s\\nActual: %s&amp;#x27;\n                    % (expected, actual))\n            raise AssertionError(error_message)\n    \n        def _error_message():\n            msg = self._format_mock_failure_message(args, kwargs)\n            return msg\n        expected = self._call_matcher(_Call((args, kwargs), two=True))\n        actual = self._call_matcher(self.call_args)\n        if actual != expected:\n            cause = expected if isinstance(expected, Exception) else None\n&amp;gt;           raise AssertionError(_error_message()) from cause\nE           AssertionError: expected call not found.\nE           Expected: PatientGenerator(&amp;#x27;data/synthetic_patients/breast_cancer/10_years/breast_cancer_10_years.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4479757056&amp;#x27;&amp;gt;, False, None)\nE           Actual: PatientGenerator(&amp;#x27;breast_cancer/10_years&amp;#x27;, &amp;lt;src.utils.config.Config object at 0x10f87dcf0&amp;gt;, False, None)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:929: AssertionError\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_invalid_zip&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_invalid_zip&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_invalid_zip&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/tmp988wy4h5.zip\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_empty_archive&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_empty_archive&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_empty_archive&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;tmp_path = PosixPath(&amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_empty_a0&amp;#x27;)\n\n    def test_patient_generator_empty_archive(tmp_path):\n        &amp;quot;&amp;quot;&amp;quot;Test loading from empty ZIP archive.&amp;quot;&amp;quot;&amp;quot;\n        empty_zip = tmp_path / &amp;quot;empty.zip&amp;quot;\n    \n        with zipfile.ZipFile(empty_zip, &amp;quot;w&amp;quot;):\n            pass  # Create empty ZIP\n    \n        try:\n            generator = PatientGenerator(str(empty_zip))\n            assert len(generator) == 0\n            with pytest.raises(ValueError):\n&amp;gt;               generator.get_random_patient()\n\ntests/unit/test_patient_generator.py:253: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.patient_generator.PatientGenerator object at 0x10add3460&amp;gt;\nexclude_ids = None\n\n    def get_random_patient(\n        self, exclude_ids: Optional[List[str]] = None\n    ) -&amp;gt; Dict[str, Any]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get a random patient bundle.\n    \n        Args:\n            exclude_ids: List of patient IDs to exclude from random selection\n    \n        Returns:\n            Random patient bundle\n        &amp;quot;&amp;quot;&amp;quot;\n        if not self._patient_files:\n&amp;gt;           raise ArchiveLoadError(&amp;quot;No patient files found in archive&amp;quot;)\nE           src.utils.patient_generator.ArchiveLoadError: No patient files found in archive\n\nsrc/utils/patient_generator.py:397: ArchiveLoadError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_empty_a0/empty.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 0 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 0 patient data files\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_non_json_files&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_non_json_files&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_non_json_files&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;create_test_zip = &amp;#x27;/private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_non_jso0/test_patients.zip&amp;#x27;\n\n    def test_patient_generator_non_json_files(create_test_zip):\n        &amp;quot;&amp;quot;&amp;quot;Test that non-JSON files in archive are ignored.&amp;quot;&amp;quot;&amp;quot;\n        # Add a non-JSON file to the test ZIP\n        with zipfile.ZipFile(create_test_zip, &amp;quot;a&amp;quot;) as zf:\n            zf.writestr(&amp;quot;patients/non_json.txt&amp;quot;, &amp;quot;not json content&amp;quot;)\n    \n        generator = PatientGenerator(create_test_zip)\n        # Should still only find the 3 JSON/NDJSON files (invalid.txt is also ignored)\n&amp;gt;       assert len(generator) == 4  # Actually 4 because invalid.txt is also parsed\nE       assert 5 == 4\nE        +  where 5 = len(&amp;lt;src.utils.patient_generator.PatientGenerator object at 0x10b035de0&amp;gt;)\n\ntests/unit/test_patient_generator.py:266: AssertionError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_patient_generator_non_jso0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 5 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 5 patient data files\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_config_resolution_failure&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_config_resolution_failure&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_config_resolution_failure&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_ndjson_malformed_lines&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_ndjson_malformed_lines&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_ndjson_malformed_lines&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_patient_generator_ndjson_malformed_lines():\n        &amp;quot;&amp;quot;&amp;quot;Test handling of malformed JSON lines in NDJSON files.&amp;quot;&amp;quot;&amp;quot;\n        # Create ZIP with NDJSON containing some invalid lines\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            zip_path = Path(tmp_dir) / &amp;quot;malformed.ndjson.zip&amp;quot;\n    \n            malformed_ndjson = (\n                json.dumps({&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;entry&amp;quot;: []})\n                + &amp;quot;\\n&amp;quot;  # Valid\n                + &amp;quot;invalid json line\\n&amp;quot;  # Invalid\n                + json.dumps({&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;valid-patient&amp;quot;})\n                + &amp;quot;\\n&amp;quot;  # Valid\n            )\n    \n            with zipfile.ZipFile(zip_path, &amp;quot;w&amp;quot;) as zf:\n                zf.writestr(&amp;quot;patients/malformed.ndjson&amp;quot;, malformed_ndjson)\n    \n            generator = PatientGenerator(str(zip_path))\n    \n            # Should load 2 valid entries despite malformed line\n&amp;gt;           patients = list(generator)\n\ntests/unit/test_patient_generator.py:308: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/utils/patient_generator.py:351: in __next__\n    patient = self._load_patient_from_file(fname)\nsrc/utils/patient_generator.py:366: in _load_patient_from_file\n    data = json.loads(content)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;json.decoder.JSONDecoder object at 0x10800bb80&amp;gt;\ns = &amp;#x27;{&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;entry&amp;quot;: []}\\ninvalid json line\\n{&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;valid-patient&amp;quot;}&amp;#x27;\n_w = &amp;lt;built-in method match of re.Pattern object at 0x107da7370&amp;gt;\n\n    def decode(self, s, _w=WHITESPACE.match):\n        &amp;quot;&amp;quot;&amp;quot;Return the Python representation of ``s`` (a ``str`` instance\n        containing a JSON document).\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n        end = _w(s, end).end()\n        if end != len(s):\n&amp;gt;           raise JSONDecodeError(&amp;quot;Extra data&amp;quot;, s, end)\nE           json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 40)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/decoder.py:340: JSONDecodeError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/tmpreqyunfv/malformed.ndjson.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 1 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 1 patient data files\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/malformed.ndjson: Extra data: line 2 column 1 (char 40)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_extract_patient_id_variations&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_extract_patient_id_variations&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_extract_patient_id_variations&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_patient_generator_extract_patient_id_variations():\n        &amp;quot;&amp;quot;&amp;quot;Test patient ID extraction from different identifier formats.&amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       from src.utils.patient_generator import _extract_patient_id\nE       ImportError: cannot import name &amp;#x27;_extract_patient_id&amp;#x27; from &amp;#x27;src.utils.patient_generator&amp;#x27; (/Users/idrdex/Documents/mcode_translator/src/utils/patient_generator.py)\n\ntests/unit/test_patient_generator.py:314: ImportError\n&#34;}], &#34;tests/unit/test_patient_generator.py::test_patient_generator_multiple_archives_config&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::test_patient_generator_multiple_archives_config&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::test_patient_generator_multiple_archives_config&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_patient_generator_multiple_archives_config():\n        &amp;quot;&amp;quot;&amp;quot;Test PatientGenerator with multiple archive configurations.&amp;quot;&amp;quot;&amp;quot;\n        # This test would typically require actual archive files, but we can test the resolution logic\n        with patch(&amp;quot;src.utils.config.Config&amp;quot;) as mock_config:\n            mock_config_instance = MagicMock()\n            mock_config.return_value = mock_config_instance\n    \n            # Test resolution with full config\n            mock_config_instance.config_data = {\n                &amp;quot;synthetic_data&amp;quot;: {\n                    &amp;quot;base_directory&amp;quot;: &amp;quot;/path/to/data&amp;quot;,\n                    &amp;quot;archives&amp;quot;: {\n                        &amp;quot;test_type&amp;quot;: {\n                            &amp;quot;test_duration&amp;quot;: {\n                                &amp;quot;url&amp;quot;: &amp;quot;http://example.com/test.zip&amp;quot;,\n                                &amp;quot;description&amp;quot;: &amp;quot;Test archive&amp;quot;,\n                            }\n                        }\n                    },\n                }\n            }\n    \n            with patch(&amp;quot;os.path.exists&amp;quot;) as mock_exists:\n                mock_exists.return_value = True\n                with patch(&amp;quot;src.utils.patient_generator.PatientGenerator&amp;quot;) as mock_gen:\n                    create_patient_generator(&amp;quot;test_type/test_duration&amp;quot;)\n    \n                    # Verify it resolved the path correctly\n                    expected_path = os.path.join(\n                        &amp;quot;/path/to/data&amp;quot;,\n                        &amp;quot;test_type&amp;quot;,\n                        &amp;quot;test_duration&amp;quot;,\n                        &amp;quot;test_type_test_duration.zip&amp;quot;,\n                    )\n&amp;gt;                   mock_gen.assert_called_with(\n                        expected_path, mock_config_instance, False, None\n                    )\n\ntests/unit/test_patient_generator.py:394: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;MagicMock name=&amp;#x27;PatientGenerator&amp;#x27; id=&amp;#x27;4479634544&amp;#x27;&amp;gt;\nargs = (&amp;#x27;/path/to/data/test_type/test_duration/test_type_test_duration.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4478526912&amp;#x27;&amp;gt;, False, None)\nkwargs = {}\nexpected = call(&amp;#x27;/path/to/data/test_type/test_duration/test_type_test_duration.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4478526912&amp;#x27;&amp;gt;, False, None)\nactual = call(&amp;#x27;test_type/test_duration&amp;#x27;, &amp;lt;src.utils.config.Config object at 0x10af41d80&amp;gt;, False, None)\n_error_message = &amp;lt;function NonCallableMock.assert_called_with.&amp;lt;locals&amp;gt;._error_message at 0x10b725f30&amp;gt;\ncause = None\n\n    def assert_called_with(self, /, *args, **kwargs):\n        &amp;quot;&amp;quot;&amp;quot;assert that the last call was made with the specified arguments.\n    \n        Raises an AssertionError if the args and keyword args passed in are\n        different to the last call to the mock.&amp;quot;&amp;quot;&amp;quot;\n        if self.call_args is None:\n            expected = self._format_mock_call_signature(args, kwargs)\n            actual = &amp;#x27;not called.&amp;#x27;\n            error_message = (&amp;#x27;expected call not found.\\nExpected: %s\\nActual: %s&amp;#x27;\n                    % (expected, actual))\n            raise AssertionError(error_message)\n    \n        def _error_message():\n            msg = self._format_mock_failure_message(args, kwargs)\n            return msg\n        expected = self._call_matcher(_Call((args, kwargs), two=True))\n        actual = self._call_matcher(self.call_args)\n        if actual != expected:\n            cause = expected if isinstance(expected, Exception) else None\n&amp;gt;           raise AssertionError(_error_message()) from cause\nE           AssertionError: expected call not found.\nE           Expected: PatientGenerator(&amp;#x27;/path/to/data/test_type/test_duration/test_type_test_duration.zip&amp;#x27;, &amp;lt;MagicMock name=&amp;#x27;Config()&amp;#x27; id=&amp;#x27;4478526912&amp;#x27;&amp;gt;, False, None)\nE           Actual: PatientGenerator(&amp;#x27;test_type/test_duration&amp;#x27;, &amp;lt;src.utils.config.Config object at 0x10af41d80&amp;gt;, False, None)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/unittest/mock.py:929: AssertionError\n&#34;}], &#34;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_full_workflow&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_full_workflow&#34;, &#34;duration&#34;: &#34;6 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_full_workflow&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;6 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_patient_generator.TestPatientGeneratorIntegration object at 0x10ae1f340&amp;gt;\n\n    def test_full_workflow(self):\n        &amp;quot;&amp;quot;&amp;quot;Test complete workflow: load -&amp;gt; iterate -&amp;gt; random -&amp;gt; specific.&amp;quot;&amp;quot;&amp;quot;\n        generator = PatientGenerator(self.test_zip, shuffle=True, seed=42)\n    \n        # 1. Test basic loading and iteration\n&amp;gt;       all_patients = list(generator)\n\ntests/unit/test_patient_generator.py:411: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/utils/patient_generator.py:351: in __next__\n    patient = self._load_patient_from_file(fname)\nsrc/utils/patient_generator.py:366: in _load_patient_from_file\n    data = json.loads(content)\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;json.decoder.JSONDecoder object at 0x10800bb80&amp;gt;\ns = &amp;#x27;{&amp;quot;resourceType&amp;quot;: &amp;quot;Bundle&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;collection&amp;quot;, &amp;quot;entry&amp;quot;: [{&amp;quot;resource&amp;quot;: {&amp;quot;resourceType&amp;quot;: &amp;quot;Patient&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;patient-12...value&amp;quot;: &amp;quot;PT123&amp;quot;}], &amp;quot;name&amp;quot;: [{&amp;quot;family&amp;quot;: &amp;quot;Johnson&amp;quot;, &amp;quot;given&amp;quot;: [&amp;quot;John&amp;quot;]}], &amp;quot;gender&amp;quot;: &amp;quot;male&amp;quot;, &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;}}]}&amp;#x27;\n_w = &amp;lt;built-in method match of re.Pattern object at 0x107da7370&amp;gt;\n\n    def decode(self, s, _w=WHITESPACE.match):\n        &amp;quot;&amp;quot;&amp;quot;Return the Python representation of ``s`` (a ``str`` instance\n        containing a JSON document).\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n        end = _w(s, end).end()\n        if end != len(s):\n&amp;gt;           raise JSONDecodeError(&amp;quot;Extra data&amp;quot;, s, end)\nE           json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 253)\n\n/opt/anaconda3/envs/mcode_translator/lib/python3.10/json/decoder.py:340: JSONDecodeError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    src.utils.patient_generator:patient_generator.py:87 Random seed set to 42 for reproducible shuffling\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_full_workflow0/test_patients.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 4 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 4 patient data files\nERROR    src.utils.patient_generator:patient_generator.py:377 Failed to load patient from patients/patients.ndjson: Extra data: line 2 column 1 (char 253)\n\n&#34;}], &#34;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_error_handling&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_error_handling&#34;, &#34;duration&#34;: &#34;10 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_patient_generator.py::TestPatientGeneratorIntegration::test_error_handling&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;10 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_patient_generator.TestPatientGeneratorIntegration object at 0x10ae1f640&amp;gt;\n\n    def test_error_handling(self):\n        &amp;quot;&amp;quot;&amp;quot;Test various error conditions.&amp;quot;&amp;quot;&amp;quot;\n        # Invalid ZIP\n        with tempfile.NamedTemporaryFile(suffix=&amp;quot;.zip&amp;quot;) as tmp:\n            tmp.write(b&amp;quot;not zip&amp;quot;)\n            tmp.flush()\n            with pytest.raises(ArchiveLoadError):\n                PatientGenerator(tmp.name)\n    \n        # Non-existent file\n        with pytest.raises(ArchiveLoadError):\n            PatientGenerator(&amp;quot;nonexistent.zip&amp;quot;)\n    \n        # Empty ZIP\n        empty_zip = Path(self.test_zip).parent / &amp;quot;empty.zip&amp;quot;\n        with zipfile.ZipFile(empty_zip, &amp;quot;w&amp;quot;):\n            pass\n        try:\n            generator = PatientGenerator(str(empty_zip))\n            assert len(generator) == 0\n            with pytest.raises(ValueError, match=&amp;quot;No available patients&amp;quot;):\n&amp;gt;               generator.get_random_patient()\n\ntests/unit/test_patient_generator.py:457: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.utils.patient_generator.PatientGenerator object at 0x10aec08b0&amp;gt;\nexclude_ids = None\n\n    def get_random_patient(\n        self, exclude_ids: Optional[List[str]] = None\n    ) -&amp;gt; Dict[str, Any]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get a random patient bundle.\n    \n        Args:\n            exclude_ids: List of patient IDs to exclude from random selection\n    \n        Returns:\n            Random patient bundle\n        &amp;quot;&amp;quot;&amp;quot;\n        if not self._patient_files:\n&amp;gt;           raise ArchiveLoadError(&amp;quot;No patient files found in archive&amp;quot;)\nE           src.utils.patient_generator.ArchiveLoadError: No patient files found in archive\n\nsrc/utils/patient_generator.py:397: ArchiveLoadError\n\n------------------------------ Captured log call -------------------------------\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/tmpxf5ln9qq.zip\nINFO     src.utils.patient_generator:patient_generator.py:144 Scanning patient data archive: /private/var/folders/c_/smr_kjws3xzcln3px90rdn600000gn/T/pytest-of-idrdex/pytest-12/test_error_handling0/empty.zip\nDEBUG    src.utils.patient_generator:patient_generator.py:152 Archive contains 0 files\nINFO     src.utils.patient_generator:patient_generator.py:157 Found 0 patient data files\n\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_init&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_init&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_init&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae1e320&amp;gt;\n\n    def test_format_mcode_display(self):\n        &amp;quot;&amp;quot;&amp;quot;Test formatting mCODE display.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        result = summarizer._format_mcode_display(&amp;quot;PrimaryCancerCondition&amp;quot;, &amp;quot;SNOMED&amp;quot;, &amp;quot;12345&amp;quot;)\n        expected = &amp;quot;PrimaryCancerCondition (SNOMED: 12345)&amp;quot;\n    \n&amp;gt;       assert result == expected\nE       AssertionError: assert &amp;#x27;(mCODE: Prim...SNOMED:12345)&amp;#x27; == &amp;#x27;PrimaryCance...NOMED: 12345)&amp;#x27;\nE         \nE         - PrimaryCancerCondition (SNOMED: 12345)\nE         ?                       ^^       -\nE         + (mCODE: PrimaryCancerCondition, SNOMED:12345)\nE         ? ++++++++                      ^^\n\ntests/unit/test_summarizer.py:28: AssertionError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae1dd80&amp;gt;\n\n    def test_format_date_simple(self):\n        &amp;quot;&amp;quot;&amp;quot;Test simple date formatting.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        result = summarizer._format_date_simple(&amp;quot;2023-01-15&amp;quot;)\n&amp;gt;       assert result == &amp;quot;Jan 15, 2023&amp;quot;\nE       AssertionError: assert &amp;#x27;2023-01-15&amp;#x27; == &amp;#x27;Jan 15, 2023&amp;#x27;\nE         \nE         - Jan 15, 2023\nE         + 2023-01-15\n\ntests/unit/test_summarizer.py:35: AssertionError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_mcode_sentence&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_mcode_sentence&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_mcode_sentence&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_demographics_sentence&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_demographics_sentence&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_demographics_sentence&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae1ff10&amp;gt;\n\n    def test_create_patient_demographics_sentence(self):\n        &amp;quot;&amp;quot;&amp;quot;Test creating patient demographics sentence.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        patient_data = {\n            &amp;quot;gender&amp;quot;: &amp;quot;female&amp;quot;,\n            &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;\n        }\n    \n        result = summarizer._create_patient_demographics_sentence(patient_data)\n    \n&amp;gt;       assert &amp;quot;female&amp;quot; in result\nE       AssertionError: assert &amp;#x27;female&amp;#x27; in &amp;#x27;Patient data not found.&amp;#x27;\n\ntests/unit/test_summarizer.py:65: AssertionError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_subject_predicate_sentence&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_subject_predicate_sentence&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_subject_predicate_sentence&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48100&amp;gt;\n\n    def test_create_trial_subject_predicate_sentence(self):\n        &amp;quot;&amp;quot;&amp;quot;Test creating trial subject predicate sentence.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        trial_data = {\n            &amp;quot;eligibility&amp;quot;: {\n                &amp;quot;criteria&amp;quot;: &amp;quot;Age &amp;gt;= 18 years&amp;quot;\n            }\n        }\n    \n&amp;gt;       result = summarizer._create_trial_subject_predicate_sentence(trial_data)\nE       TypeError: McodeSummarizer._create_trial_subject_predicate_sentence() missing 2 required positional arguments: &amp;#x27;mcode_element&amp;#x27; and &amp;#x27;predicate&amp;#x27;\n\ntests/unit/test_summarizer.py:78: TypeError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48280&amp;gt;\nmock_mcode_sentence = &amp;lt;MagicMock name=&amp;#x27;_create_mcode_sentence&amp;#x27; id=&amp;#x27;4479747168&amp;#x27;&amp;gt;\nmock_demographics = &amp;lt;MagicMock name=&amp;#x27;_create_patient_demographics_sentence&amp;#x27; id=&amp;#x27;4479756816&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.services.summarizer.McodeSummarizer._create_patient_demographics_sentence&amp;#x27;)\n    @patch(&amp;#x27;src.services.summarizer.McodeSummarizer._create_mcode_sentence&amp;#x27;)\n    def test_create_patient_summary(self, mock_mcode_sentence, mock_demographics):\n        &amp;quot;&amp;quot;&amp;quot;Test creating patient summary.&amp;quot;&amp;quot;&amp;quot;\n        mock_demographics.return_value = &amp;quot;Female patient born in 1980&amp;quot;\n        mock_mcode_sentence.return_value = &amp;quot;Has breast cancer&amp;quot;\n    \n        summarizer = McodeSummarizer()\n    \n        patient_data = {\n            &amp;quot;gender&amp;quot;: &amp;quot;female&amp;quot;,\n            &amp;quot;birthDate&amp;quot;: &amp;quot;1980-01-01&amp;quot;,\n            &amp;quot;conditions&amp;quot;: [\n                {\n                    &amp;quot;code&amp;quot;: {\n                        &amp;quot;coding&amp;quot;: [\n                            {\n                                &amp;quot;system&amp;quot;: &amp;quot;http://snomed.info/sct&amp;quot;,\n                                &amp;quot;code&amp;quot;: &amp;quot;254837009&amp;quot;,\n                                &amp;quot;display&amp;quot;: &amp;quot;Malignant neoplasm of breast&amp;quot;\n                            }\n                        ]\n                    }\n                }\n            ]\n        }\n    \n&amp;gt;       result = summarizer.create_patient_summary(patient_data)\n\ntests/unit/test_summarizer.py:109: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10b038be0&amp;gt;\npatient_data = {&amp;#x27;birthDate&amp;#x27;: &amp;#x27;1980-01-01&amp;#x27;, &amp;#x27;conditions&amp;#x27;: [{&amp;#x27;code&amp;#x27;: {&amp;#x27;coding&amp;#x27;: [{&amp;#x27;code&amp;#x27;: &amp;#x27;254837009&amp;#x27;, &amp;#x27;display&amp;#x27;: &amp;#x27;Malignant neoplasm of breast&amp;#x27;, &amp;#x27;system&amp;#x27;: &amp;#x27;http://snomed.info/sct&amp;#x27;}]}}], &amp;#x27;gender&amp;#x27;: &amp;#x27;female&amp;#x27;}\ninclude_dates = True\n\n    def create_patient_summary(\n        self, patient_data: Dict[str, Any], include_dates: bool = None\n    ) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive patient summary optimized for NLP entity extraction and clinical trial matching.\n    \n        Args:\n            patient_data: A dictionary representing a patient&amp;#x27;s FHIR bundle.\n            include_dates: Whether to include dates in the summary. If None, uses instance default.\n    \n        Returns:\n            A natural language summary of the patient&amp;#x27;s mCODE data optimized for clinical trial matching.\n    \n        Raises:\n            ValueError: If the patient data is missing required fields.\n        &amp;quot;&amp;quot;&amp;quot;\n        if include_dates is None:\n            include_dates = self.include_dates\n        if not patient_data or &amp;quot;entry&amp;quot; not in patient_data:\n&amp;gt;           raise ValueError(&amp;quot;Patient data is missing or not in the expected format.&amp;quot;)\nE           ValueError: Patient data is missing or not in the expected format.\n\nsrc/services/summarizer.py:738: ValueError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48400&amp;gt;\nmock_mcode_sentence = &amp;lt;MagicMock name=&amp;#x27;_create_mcode_sentence&amp;#x27; id=&amp;#x27;4479751440&amp;#x27;&amp;gt;\nmock_subject_predicate = &amp;lt;MagicMock name=&amp;#x27;_create_trial_subject_predicate_sentence&amp;#x27; id=&amp;#x27;4555174768&amp;#x27;&amp;gt;\n\n    @patch(&amp;#x27;src.services.summarizer.McodeSummarizer._create_trial_subject_predicate_sentence&amp;#x27;)\n    @patch(&amp;#x27;src.services.summarizer.McodeSummarizer._create_mcode_sentence&amp;#x27;)\n    def test_create_trial_summary(self, mock_mcode_sentence, mock_subject_predicate):\n        &amp;quot;&amp;quot;&amp;quot;Test creating trial summary.&amp;quot;&amp;quot;&amp;quot;\n        mock_subject_predicate.return_value = &amp;quot;For patients aged 18+&amp;quot;\n        mock_mcode_sentence.return_value = &amp;quot;Studies breast cancer&amp;quot;\n    \n        summarizer = McodeSummarizer()\n    \n        trial_data = {\n            &amp;quot;nct_id&amp;quot;: &amp;quot;NCT12345678&amp;quot;,\n            &amp;quot;title&amp;quot;: &amp;quot;Test Trial&amp;quot;,\n            &amp;quot;conditions&amp;quot;: [&amp;quot;Breast Cancer&amp;quot;],\n            &amp;quot;eligibility&amp;quot;: {\n                &amp;quot;criteria&amp;quot;: &amp;quot;Age &amp;gt;= 18&amp;quot;\n            }\n        }\n    \n&amp;gt;       result = summarizer.create_trial_summary(trial_data)\n\ntests/unit/test_summarizer.py:132: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10b0392a0&amp;gt;\ntrial_data = {&amp;#x27;conditions&amp;#x27;: [&amp;#x27;Breast Cancer&amp;#x27;], &amp;#x27;eligibility&amp;#x27;: {&amp;#x27;criteria&amp;#x27;: &amp;#x27;Age &amp;gt;= 18&amp;#x27;}, &amp;#x27;nct_id&amp;#x27;: &amp;#x27;NCT12345678&amp;#x27;, &amp;#x27;title&amp;#x27;: &amp;#x27;Test Trial&amp;#x27;}\n\n    def create_trial_summary(self, trial_data: Dict[str, Any]) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive clinical trial summary in natural language format.\n    \n        Args:\n            trial_data: Clinical trial data from ClinicalTrials.gov API\n    \n        Returns:\n            str: Natural language summary of the clinical trial\n    \n        Raises:\n            ValueError: If trial data is missing required fields\n        &amp;quot;&amp;quot;&amp;quot;\n        if not trial_data or not trial_data.get(&amp;quot;protocolSection&amp;quot;):\n&amp;gt;           raise ValueError(&amp;quot;Trial data is missing or not in the expected format.&amp;quot;)\nE           ValueError: Trial data is missing or not in the expected format.\n\nsrc/services/summarizer.py:326: ValueError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_complete&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_complete&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_complete&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_incomplete&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_incomplete&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_check_trial_data_completeness_incomplete&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;No log output captured.&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary_empty_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary_empty_data&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_patient_summary_empty_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48940&amp;gt;\n\n    def test_create_patient_summary_empty_data(self):\n        &amp;quot;&amp;quot;&amp;quot;Test creating patient summary with empty data.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n&amp;gt;       result = summarizer.create_patient_summary({})\n\ntests/unit/test_summarizer.py:171: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10aed5240&amp;gt;\npatient_data = {}, include_dates = True\n\n    def create_patient_summary(\n        self, patient_data: Dict[str, Any], include_dates: bool = None\n    ) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive patient summary optimized for NLP entity extraction and clinical trial matching.\n    \n        Args:\n            patient_data: A dictionary representing a patient&amp;#x27;s FHIR bundle.\n            include_dates: Whether to include dates in the summary. If None, uses instance default.\n    \n        Returns:\n            A natural language summary of the patient&amp;#x27;s mCODE data optimized for clinical trial matching.\n    \n        Raises:\n            ValueError: If the patient data is missing required fields.\n        &amp;quot;&amp;quot;&amp;quot;\n        if include_dates is None:\n            include_dates = self.include_dates\n        if not patient_data or &amp;quot;entry&amp;quot; not in patient_data:\n&amp;gt;           raise ValueError(&amp;quot;Patient data is missing or not in the expected format.&amp;quot;)\nE           ValueError: Patient data is missing or not in the expected format.\n\nsrc/services/summarizer.py:738: ValueError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary_empty_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary_empty_data&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_create_trial_summary_empty_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48af0&amp;gt;\n\n    def test_create_trial_summary_empty_data(self):\n        &amp;quot;&amp;quot;&amp;quot;Test creating trial summary with empty data.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n&amp;gt;       result = summarizer.create_trial_summary({})\n\ntests/unit/test_summarizer.py:180: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = &amp;lt;src.services.summarizer.McodeSummarizer object at 0x10af06bf0&amp;gt;\ntrial_data = {}\n\n    def create_trial_summary(self, trial_data: Dict[str, Any]) -&amp;gt; str:\n        &amp;quot;&amp;quot;&amp;quot;\n        Generate a comprehensive clinical trial summary in natural language format.\n    \n        Args:\n            trial_data: Clinical trial data from ClinicalTrials.gov API\n    \n        Returns:\n            str: Natural language summary of the clinical trial\n    \n        Raises:\n            ValueError: If trial data is missing required fields\n        &amp;quot;&amp;quot;&amp;quot;\n        if not trial_data or not trial_data.get(&amp;quot;protocolSection&amp;quot;):\n&amp;gt;           raise ValueError(&amp;quot;Trial data is missing or not in the expected format.&amp;quot;)\nE           ValueError: Trial data is missing or not in the expected format.\n\nsrc/services/summarizer.py:326: ValueError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display_edge_cases&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display_edge_cases&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_mcode_display_edge_cases&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48ca0&amp;gt;\n\n    def test_format_mcode_display_edge_cases(self):\n        &amp;quot;&amp;quot;&amp;quot;Test mCODE display formatting edge cases.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        # Empty values\n        result = summarizer._format_mcode_display(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;)\n&amp;gt;       assert result == &amp;quot; (: )&amp;quot;\nE       AssertionError: assert &amp;#x27;&amp;#x27; == &amp;#x27; (: )&amp;#x27;\nE         \nE         -  (: )\n\ntests/unit/test_summarizer.py:191: AssertionError\n&#34;}], &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple_edge_cases&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple_edge_cases&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_summarizer.py::TestMcodeSummarizer::test_format_date_simple_edge_cases&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_summarizer.TestMcodeSummarizer object at 0x10ae48e50&amp;gt;\n\n    def test_format_date_simple_edge_cases(self):\n        &amp;quot;&amp;quot;&amp;quot;Test date formatting edge cases.&amp;quot;&amp;quot;&amp;quot;\n        summarizer = McodeSummarizer()\n    \n        # None input\n        result = summarizer._format_date_simple(None)\n&amp;gt;       assert result == &amp;quot;None&amp;quot;\nE       AssertionError: assert &amp;#x27;&amp;#x27; == &amp;#x27;None&amp;#x27;\nE         \nE         - None\n\ntests/unit/test_summarizer.py:203: AssertionError\n&#34;}], &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_initialization&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_initialization&#34;, &#34;duration&#34;: &#34;461 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_initialization&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;461 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nINFO     src.utils.api_manager:api_manager.py:312 APIManager initialized with config TTL: 0 seconds\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trials_processor&amp;#x27; at .api_cache/trials_processor with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;mcode_extraction&amp;#x27; at .api_cache/mcode_extraction with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trial_summaries&amp;#x27; at .api_cache/trial_summaries with TTL 0\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_execute_success&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_execute_success&#34;, &#34;duration&#34;: &#34;440 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_execute_success&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;440 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_workflow.TestClinicalTrialsProcessorWorkflow object at 0x10ae49840&amp;gt;\n\n    def test_execute_success(self):\n        &amp;quot;&amp;quot;&amp;quot;Test successful execution.&amp;quot;&amp;quot;&amp;quot;\n        processor = ClinicalTrialsProcessorWorkflow(config={})\n    \n        # Mock the execute method to return a successful result\n&amp;gt;       with patch.object(processor, &amp;#x27;execute&amp;#x27;) as mock_execute:\nE       NameError: name &amp;#x27;patch&amp;#x27; is not defined\n\ntests/unit/test_workflow.py:31: NameError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nINFO     src.utils.api_manager:api_manager.py:312 APIManager initialized with config TTL: 0 seconds\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trials_processor&amp;#x27; at .api_cache/trials_processor with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;mcode_extraction&amp;#x27; at .api_cache/mcode_extraction with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trial_summaries&amp;#x27; at .api_cache/trial_summaries with TTL 0\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_process_single_trial_method&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_process_single_trial_method&#34;, &#34;duration&#34;: &#34;551 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_process_single_trial_method&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;551 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nINFO     src.utils.api_manager:api_manager.py:312 APIManager initialized with config TTL: 0 seconds\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trials_processor&amp;#x27; at .api_cache/trials_processor with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;mcode_extraction&amp;#x27; at .api_cache/mcode_extraction with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trial_summaries&amp;#x27; at .api_cache/trial_summaries with TTL 0\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_validate_trial_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_validate_trial_data&#34;, &#34;duration&#34;: &#34;424 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_validate_trial_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;424 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_workflow.TestClinicalTrialsProcessorWorkflow object at 0x10ae1e020&amp;gt;\n\n    def test_validate_trial_data(self):\n        &amp;quot;&amp;quot;&amp;quot;Test trial data validation.&amp;quot;&amp;quot;&amp;quot;\n        processor = ClinicalTrialsProcessorWorkflow(config={})\n    \n        # Valid trial data\n        valid_trial = {\n            &amp;quot;protocolSection&amp;quot;: {\n                &amp;quot;identificationModule&amp;quot;: {&amp;quot;nctId&amp;quot;: &amp;quot;NCT123&amp;quot;}\n            }\n        }\n    \n        is_valid = processor.validate_trial_data(valid_trial)\n&amp;gt;       assert is_valid is True\nE       assert False is True\n\ntests/unit/test_workflow.py:59: AssertionError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nINFO     src.utils.api_manager:api_manager.py:312 APIManager initialized with config TTL: 0 seconds\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trials_processor&amp;#x27; at .api_cache/trials_processor with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;mcode_extraction&amp;#x27; at .api_cache/mcode_extraction with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trial_summaries&amp;#x27; at .api_cache/trial_summaries with TTL 0\nWARNING  ClinicalTrialsProcessorWorkflow:trials_processor_workflow.py:1209 Trial NCT123 missing eligibility criteria\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_get_processing_stats&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_get_processing_stats&#34;, &#34;duration&#34;: &#34;721 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestClinicalTrialsProcessorWorkflow::test_get_processing_stats&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;721 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;self = &amp;lt;test_workflow.TestClinicalTrialsProcessorWorkflow object at 0x10ae49510&amp;gt;\n\n    def test_get_processing_stats(self):\n        &amp;quot;&amp;quot;&amp;quot;Test getting processing statistics.&amp;quot;&amp;quot;&amp;quot;\n        processor = ClinicalTrialsProcessorWorkflow(config={&amp;quot;test&amp;quot;: &amp;quot;config&amp;quot;})\n    \n        stats = processor.get_processing_stats()\n    \n        assert isinstance(stats, dict)\n&amp;gt;       assert &amp;quot;config&amp;quot; in stats\nE       AssertionError: assert &amp;#x27;config&amp;#x27; in {&amp;#x27;status&amp;#x27;: &amp;#x27;pipeline_not_initialized&amp;#x27;}\n\ntests/unit/test_workflow.py:68: AssertionError\n\n------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nINFO     src.utils.api_manager:api_manager.py:312 APIManager initialized with config TTL: 0 seconds\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trials_processor&amp;#x27; at .api_cache/trials_processor with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;mcode_extraction&amp;#x27; at .api_cache/mcode_extraction with TTL 0\nINFO     src.utils.api_manager:api_manager.py:32 Initialized API cache for namespace &amp;#x27;trial_summaries&amp;#x27; at .api_cache/trial_summaries with TTL 0\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_initialization&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_initialization&#34;, &#34;duration&#34;: &#34;418 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_initialization&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;418 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_execute_method_exists&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_execute_method_exists&#34;, &#34;duration&#34;: &#34;458 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestTrialsFetcherWorkflow::test_execute_method_exists&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;458 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_initialization&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_initialization&#34;, &#34;duration&#34;: &#34;445 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_initialization&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;445 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}], &#34;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_execute_method_exists&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_execute_method_exists&#34;, &#34;duration&#34;: &#34;509 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/unit/test_workflow.py::TestPatientsProcessorWorkflow::test_execute_method_exists&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;509 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\nDEBUG    urllib3.connectionpool:connectionpool.py:1049 Starting new HTTPS connection (1): core.heysol.ai:443\nDEBUG    urllib3.connectionpool:connectionpool.py:544 https://core.heysol.ai:443 &amp;quot;POST /api/v1/mcp?source=mcode_translator HTTP/1.1&amp;quot; 200 None\n\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;test_report.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>