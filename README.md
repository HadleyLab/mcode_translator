# mCODE Translator v2.0

> **High-performance clinical trial data processing pipeline that extracts and maps eligibility criteria to standardized mCODE elements using evidence-based LLM processing.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone https://github.com/HadleyLab/mcode_translator.git
cd mcode_translator
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env with your OPENAI_API_KEY or DEEPSEEK_API_KEY

# Process clinical trial data
python mcode_translator.py data/selected_breast_cancer_trials.json -o results.json
```

## ğŸ’» Usage

### mCODE Translator - Process Trial Data

```bash
# Basic processing
python mcode_translator.py trial_data.json -o results.json

# With specific model and prompt (using shorthands)
python mcode_translator.py trial_data.json -m deepseek-coder -p direct_mcode_evidence_based_concise -o results.json

# Batch processing
python mcode_translator.py trials_batch.json --batch -o batch_results.json
```

### mCODE Fetcher - Search and Process Trials

```bash
# Search and fetch trials
python mcode_fetcher.py --condition "breast cancer" --limit 10 -o results.json

# Search with concurrent mCODE processing
python mcode_fetcher.py --condition "lung cancer" --concurrent --process \
  --workers 8 -m deepseek-coder -o processed_results.json

# Fetch specific trials with processing
python mcode_fetcher.py --nct-ids "NCT001,NCT002,NCT003" --process -m deepseek-coder -p direct_mcode_evidence_based_concise -o trials.json

# Count available studies
python mcode_fetcher.py --condition "cancer" --count-only
```

### mCODE Optimize - Cross-Validation Testing

```bash
# Full optimization across all promptÃ—model combinations
python mcode_optimize.py --concurrent --detailed-report

# Quick optimization with limited combinations
python mcode_optimize.py --max-combinations 10 --concurrent

# Test specific models and prompts
python mcode_optimize.py --models deepseek-coder,gpt-4o \
  --prompts direct_mcode_evidence_based_concise,direct_mcode_simple \
  --detailed-report

# Quick test with representative sample
python quick_cross_validation.py
```

### Python API

```python
from src.pipeline import McodePipeline

# Initialize pipeline with evidence-based processing
pipeline = McodePipeline(
    prompt_name="direct_mcode_evidence_based_concise"
)

# Process trial data
result = pipeline.process_clinical_trial(trial_data)
print(f"Generated {len(result.mcode_mappings)} mCODE mappings")
print(f"Quality Score: {result.validation_results['compliance_score']:.3f}")
```

## ğŸ¯ Key Features

- **99.1% Quality Score** - Evidence-based processing with strict textual fidelity
- **Single-Step Pipeline** - Direct clinical text to mCODE mapping
- **Conservative Mapping** - Quality over quantity approach
- **Source Provenance** - Complete audit trail for all mappings
- **Production Ready** - Clean architecture with comprehensive error handling

## ğŸ“ Project Structure

```
mcode_translator/
â”œâ”€â”€ mcode_translator.py             # Main CLI - Process trial data with mCODE
â”œâ”€â”€ mcode_fetcher.py                # Fetcher CLI - Search and fetch trials
â”œâ”€â”€ src/pipeline/                   # Core processing components
â”‚   â”œâ”€â”€ mcode_pipeline.py          # Main mCODE processing pipeline
â”‚   â”œâ”€â”€ mcode_mapper.py            # LLM-based mCODE mapping
â”‚   â”œâ”€â”€ concurrent_fetcher.py      # Concurrent processing engine
â”‚   â””â”€â”€ fetcher.py                 # Core trial fetching functions
â”œâ”€â”€ prompts/                        # Evidence-based prompt templates
â”œâ”€â”€ models/                         # LLM model configurations
â”œâ”€â”€ data/                           # Configuration and reference data
â””â”€â”€ tests/                          # Test suite
```

## ğŸ“ˆ Quality Metrics

| Metric | Score | Improvement |
|--------|-------|-------------|
| Overall Quality | 98.9% | +8.5 points |
| Source Text Fidelity | 99.2% | +8.5 points |
| Average Confidence | 98.2% | +8.6 points |
| Mapping Efficiency | 44.9% reduction in over-mapping | Quality over quantity |

*Validated across 5 comprehensive breast cancer clinical trials*

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Required: Choose one LLM provider
OPENAI_API_KEY=your_openai_api_key
DEEPSEEK_API_KEY=your_deepseek_api_key
```

### Model Settings (models/models_config.json)

```json
{
  "models": {
    "deepseek-coder": {
      "provider": "deepseek",
      "max_tokens": 8000,
      "temperature": 0.1
    }
  }
}
```

## ğŸ§ª Testing

```bash
python -m pytest tests/
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

**mCODE Translator v2.0** - Clinical trial data to standardized mCODE elements with 99.1% quality score.